[
["index.html", "Introduction to Spatial Network Forecast with R Preface Motivations and Approach Prerequisites Conventions Online Version Acknowledgements About the Authors", " Introduction to Spatial Network Forecast with R Laurent Lacaze Santos &amp; Francisco Salgado Castillo July 2019 First Edition Preface Welcome to our book on Introduction to Spatial Network Forecast with R. This tutorial book is intended to provide a comprehensive introduction to forecasting strategies of network data. We use R throughout the book, and we intend students to learn how to forecast with R. The concepts and tools are taught on the same frame, so you will learn the theory and the implementation at the same time. R is free and it is a broad-based application for statistical analysis in general, not just for forecasting. A good introduction to R language can be found in: https://cran.r-project.org/doc/manuals/r-release/R-intro.pdf. The R Studio online learning resources have tutorials and links on various aspects of the R language and R Studio environment: https://www.rstudio.com/online-learning/. And finally, the book R for Data Science presents an excellent introduction of data analytics with R and can be found in https://bookdown.org. Most quantitative prediction problems use either time series data (collected at regular intervals over time) or cross-sectional data (collected at a single point in time). In this book we are concerned with forecasting future data, and we concentrate on the time series domain. Anything that is observed sequentially over time is a time series. In this book, we will only consider time series that are observed at regular intervals of time (e.g., hourly, daily, weekly, monthly, quarterly, annually). Irregularly spaced time series can also occur, but are beyond the scope of this book. A number of practical decisions problems in business fall into a category known as network flow problems. These problems share a common characteristics: they can be described in a graphical form knows as a network. This book focusses on the short term forecasting1 of transportation network flow problem. Travel time data collected using automatic number plate recognition data as part of Transport for London’ s London Congestion Analysis Project2 are applied throughout the book. More advanced or detailed treatment of the methods applied in the book can be found in the reference section of each chapter. Comments, feedbacks or any additional information, please send an email to ucesljl@ucl.ac.uk. Motivations and Approach Network science literature has gained popularity to a broad spectrum of areas such as engineering, economics and geography over the last decade. Most of the approaches focus on analysing characteristics and opportunities of enhancing the effectiveness of a network system. Many researches centre on questions regarding structural and dynamics issues, as well as their implications to the outcomes of a network system. This book demonstrates the mechanics of modeling and solving network problems and statistical forecasting. The document drives the students to the methodologies with a description of each applied method. Furthermore, by means of practical sections, the book offers a comprehensive pathway to modeling and solving techniques required to each forecasting methodology. The book also aims to equip the reader with an understanding of the principles underlying each methodology, the essential of data analytics and visualisation technique. Additionally, a full charter is devoted to principles of exploratory spatio-temporal data analysis and visualisation in order to offer the foundations of spatio-temporal autocorrelation and spatial-temporal modelling, an essential part to tackle machine learning and spatio-temporal series analysis and forecasting. By the end of the book, the student would have the essential skills to apply time series modelling to forecast travel times of network data of urban road, generate insights into spatio-temporal dataset to uncover patterns in spatial, temporal and spatio-temporal data and to evaluate methodologies and discuss strengths and weaknesses of each model. Prerequisites To get the most out of this book, it is assumed that you are numerically literate and you also have some programming experience already. To run the code of this book, you need fisrt to install RStudio and have tidyverse packages installed. RStudio is an integrated development environment, or IDE, for R programming. You can download and install it from http://www.rstudio.com/download. Further instructions of how to set-up R can be accessed in the Appendix. Base packages are preinstalled and can be loaded using library('package_name'). Libraries from a repository called CRAN (the Comprehensive R Archive Network, https://cran.r-project.org) need to be installed first using install.packages('package_name') before they can be loaded using library. Conventions Throughout the book we use a consistent set of conventions to refer to code: Functions are in a code font and followed by parentheses, like sum(), or mean(). Used for program listings to refer to program elements such as variable or function names, databases, data types, element variables, statements and keyword, are in a code font, without parentheses, like data or x. If we want to make it clear what package an object comes from, we’ll use the package name followed by two colons, like dplyr::mutate(). We also employ some specific typographical conventions: italic indicates new terms, reference sources, filenames and file extensions and name of R packages. Constant width italic shows text that should be replaced with use-supplied values or by values determined by context. For your convenience (for example, when you want to copy and run the code), we do not add prompts (&gt; and +) to R source code in this book and outputs are preceded by ##. These are the results of running the code and should not be typed in the console or included in your scripts. You should see these outputs in your R Studio console when you run the code. Online Version An online version of this book is available at https://laurentlsantos.github.io/forecasting/. The book has been compiled using bookdown3 and will continue to evolve in between reprints of the physical book. Acknowledgements We are particularly indebted to those at University College London (UCL) who have been generous with their time and expertise. From the Department of Civil, Environmental and Geomatic Engineering (CEGE), we are extremely grateful to professor James Haworth from SpaceTimeLab , who introduced and equipped with the techniques of spatial-temporal data analysis and big data analytics. We are also thankful to James Haworth for the data applied throughout the book. We also must thank Tao Cheng, professor in geoinformatics at UCL, for all the skills and on-going academic mentorship in the field of spatial analysis and geocomputation. To Mohamed Ibrahim, PhD researcher at SpaceTimeLab, for all support and debugging with R programming. We must also extent our thanks to Transport of London which is ultimately responsible for the London Congestion Analysis Project (LCAP), a significant achievement in understanding and managing congestion in a such fascinating city. About the Authors Laurent Lacaze Santos is a geographer and business manager with passion for spatial analytics and location intelligence. Having achieved a solid experience in project management, operations management and consultancy, he extended his expertise to geographic information science, remote sensing and quantitative geography, and took part in projects of GIS infrastructure, environmental planning and business intelligence. Laurent holds a master’ s degree in geosciences (governance of risk and resources) with emphasis on remote sensing and GIS from Heidelberg University and is a master’s candidate on geospatial sciences (geographic information science and computing) at University College London (UCL). Email address: laurent.santos.18@ucl.ac.uk Follow me on twitter: http://twitter.com/laurentlsantos Francisco Salgado Castillo is an experienced engineer in the fields of environment, urban planning and geospatial analysis. He gained experience in projects of environment and spatial analysis at the Instituto de Estudios de Regimen Seccional del Ecuador (IERSE) / Universidad del Azuay for sustainable development in Ecuador. Francisco holds a double undergraduate degree in systems and telematics engineer and electronic engineer and also holds a Master’s in spatio-temporal analytics and big data mining at University College London (UCL). Email address: chescosalgado@gmail.com i.e. the forecast of intra-day traffic.↩ A full report on understanding and managing congestion in London is available on http://content.tfl.gov.uk/understanding-and-managing-congestion-in-london.pdf↩ For more information, see https://bookdown.org↩ "],
["getting-started.html", "1 Getting Started 1.1 Introduction 1.2 Network 1.3 Forecasting 1.4 Forecasting Data and Methods 1.5 Chapter Summary", " 1 Getting Started 1.1 Introduction Complex network attracts increasing attention in many fields and analytics tools offer tool for empirically investigating the structure and evolution of network structures. Since the latter period of the 20th century, there has been an explosion of research into the structure and function of complex networks. In this chapter, you will be introduced to two main topics that will ground all the following chapters of the book: 1. network and 2. forecasting techniques for spatial network data. To understand the fundamentals of these two broad topics, the basic concepts and terminology of networks are introduced. Particular attention is paid to spatial networks in order to limit the scope of the review. Methods for forecasting network data are subsequently introduced. 1.2 Network 1.2.1 Network: fundamentals The network science literature provides a diverse range of tools and techniques for analysing and enhancing our understanding of the structure and large-scale statistical properties of networks. A network is a set of discrete items, called nodes, with connections between them, called edges. A network is typically represented as a graph G = (N, E), with a set of N nodes and E edges. In road network, each intersection is a node and the road segments between intersections are edges. A bimodal network (also called bipartite network) is one which connects two varieties of things, whereas unimodal (or unimode) network connects nodes with same characteristics (see figure 1.1) Figure 1.1: Topology of Networks (source: Wasserman and Faust 1994). To analyse and model a network process, a network is represented mathematically by a matrix describing the list of adjacency edges of incoming and outgoing vectors (Figure 1.2). Figure 1.2: The London Congestion Analysis Project Road Links. In broad terms, networks can be divides into four categories (see Newman 2003) Social Network: a set of people (or animals) or groups of people with associations between them. The core foundation of social network analysis is to understand the effect of social networks on social processes (e.g., friendship networks, academic collaborations, business relationships and animal behavior). Information Network: networks of information flow. Pieces of information are stored at the nodes. The edges between nodes signify connections between information sources (e.g., academic citation networks and world wide web). Biological Network: networks of biological systems, where the nodes are biological entities and the edges are connections between them (e.g., metabolic networks, neural networks and food webs). Technological Network: man-made, physical networks, designed to transport people, resources or commodities (e.g., transportation networks of roads, rail and airlines). The scales of analysis of a network is usually decomposed in three different levels of description. Network theory has long been applied to transportation systems studies which seek to understand the dynamics of physical networks designed to transport people and to evaluate impacts on transportation system performance (Xie and Levinson 2007). Figure 1.3: Network Structures (source: Borgatti and Johnson 2013). 1.2.2 Road Network and Travel Time as a Variable for Transportation Road network is among the most recognisable and studied types of network. Traffic on urban networks is fundamentally different to traffic on highways, primarily because it is interrupted by traffic signals at intersections. The underlying factor that determines the speed of the traffic stream on both highways and urban roads is interaction between the demand for road space and the supply of road space, known as capacity. Congestion is characterised by slow moving stop and go traffic and is very frustrating and costly for road users. Travel time (TT) is one of the most important variables in transportation. Accurate travel-time prediction is crucial to the development of transportation systems and advanced traveler information systems. Travel time prediction in conjunction with traffic network analytics fosters route-guidance systems and supports transportation planning and operations. Some of the common applications of traffic network analysis are listed on table 1.1. Table 1.1: Traffic Network Analysis Applications (source: Ordnance-Survey 2017) Application Description Catchment-area analysis Command and control for emergency services GIS analysis, indexing and mapping Highway design, planning and engineering In-vehicle navigation and guidance Derivation of street gazetteers Logistics management Real-time traffic control Road and highway maintenance Road-user charging schemes Route planning and vehicle tracking Scheduling and delivery Site location Traffic management The study of road network structure predates the field of complex networks and has long been of interest due to its inherent impact on transportation system performance. The factor that determines the speed of the traffic stream on both highways and urban roads is interaction between the demand for road space and the supply of road space known as capacity. Congestion is characterised by slow moving stop and go traffic. Forecasting models of network data have traditionally focussed on time series approaches. In such approaches, the network structure is ignored and each of the time series at the nodes is forecast independently. More recent approaches use network structures to construct spatio-temporal models that capture the spatio-temporal dependency between locations where network data are observed. Increasingly, researchers and practitioners are turning towards less conventional techniques, often with their roots in the machine learning (ML) and data mining communities, that are better equipped to deal with the heterogeneous, nonlinear and multi-scale properties of large scale network datasets. For the forecast of urban network, observations are made at regular intervals using traffic static sensors, such as loop detectors or automatic number plate recognition (ANPR) cameras, which measure traffic conditions at a fixed point, or along a section of road called a link. These sensors are distributed across the network, and when considered together, form a space time series that describes the network process (e.g. transportation systems, the internet and electrical grids). Observations are made at regular intervals using sensors, which form time series. These sensors are distributed across the network, and when considered together, form a space time series that describes the network process (e.g. transportation systems, the internet and electrical grids). The majority of the literature on traffic forecasting focuses on data-driven approaches. In such approaches, statistical relationships are extracted from historical data, and used to forecast future traffic conditions using information about the current traffic conditions. Spatial network data combines the challenges of modelling time series and spatial series, which are autocorrelation, nonstationarity, heterogeneity and nonlinearity (see Haworth 2013). The choice of method depends on the dataset and an explanatory spatio-temporal data analysis must be done to better understand the characteristics of the data. Table 1.2: Forecast of Traffic Data (source: Haworth 2013) Problem Variable Description Traffic flow forecasting Flow (q) The rate at which vehicles pass a point during a given period. Travel time forecasting Travel time (TT) The time taken to travel between two fixed points. 1.3 Forecasting Forecasting is a technique that uses historical data as inputs to make informed estimates that are predictive in determining the direction of future trends. A good definition is ofered by Hyndman and Athanasopoulos (2018). Forecasting: is about predicting the future as accurately as possible, given all of the information available, including historical data and knowledge of any future events that might impact the forecasts. (Hyndman and Athanasopoulos 2018) 1.3.1 Space-time Series Forecasting A time series is a set of observations on quantitative variables collected over time. In conjunction with a spatial component, the series turns out to be a space-time series of values for a quantitative variable over time. Most businesses keep track of a number of time series and geographic information science also rely on data series to forecast environmental risk, weather forecast, land-cover changes over time and rout network forecast. There are many forms to predict the behaviour of a dependent variable using one or more independent variables that are believed to be related to the dependent variable in a causal fashion. Although we can sometimes use regression models to predictive analysis, its is not always possible to do so. For example, if we do not know which causal independent variables are influencing a particular time series variable, we cannot build a regression model. And even if we do have some idea which causal variables are affecting a time series, there might not be any data available for those variables. Moreover, even if the estimated regression function fits the data well, we might have to forecast the values of the causal independent variables in order to estimate future values of the dependent (time series) variable. That said, it might be difficult or even undesirable to forecast time series data using a regression model. In these cases, it is advisable to use a time series forecasting method in which we analyse the past behaviour of the time series variables in order to predict its future behaviour (for further details and examples of forecasting methods, we recommend Hyndman and Athanasopoulos 2018). The predictability of an event depends on several factors: how well we understand the factors that contribute to it; how much data is available; whether the forecast can somehow affect the process that is being forecast. Techniques that analyse the past behaviour of a time series variable to predict the future are referred to as extrapolation model. The goal of an extrapolation model is to identify a function f() that produces accurate forecasts of future values of the time series variable. Many methods have been developed to model time series data. A common approach involves trying several modeling techniques on a given data set and evaluating how well they explain the past behavior of the time series variable. The most used accuracy measures are the mean absolute deviation (MAD), the mean absolute percent error (MAPE), the mean square error (MSE), ant the root mean square error (RMSE). In each of these formulas \\(Y_i\\) represents the actual value of \\(i^{th}\\) observation in the time series and \\({\\hat {Y_{i}}}\\) is the forecasted value for this observation. These quantities measure the differences between the actual values in the time series and the predicted, or fitted, values generated by the forecasting technique. These quantities are defined as follows: \\[\\begin{equation} {\\displaystyle \\operatorname {MAD} ={\\frac {1}{n}}\\sum _{i=1}^{n}|Y_{i}-{\\hat {Y_{i}}}|.} \\tag{1.1} \\end{equation}\\] MAD (1.1) is the average of the absolute deviations or the positive difference of the given data and that certain value (generally central values). It is a summary statistic of statistical dispersion or variability. \\[\\begin{equation} {\\displaystyle {\\mbox{MAPE}}={\\frac {100\\%}{n}}\\sum _{t=1}^{n}\\left|{\\frac {Y_{i}-{\\hat {Y_{i}}}}{Y_{i}}}\\right|} \\tag{1.2} \\end{equation}\\] The difference between \\(Y_i\\) and \\({\\hat {Y_{i}}}\\) is divided by the actual value \\(Y_i\\) again. The absolute value in this calculation is summed for every forecasted point in time and divided by the number of fitted points n. Multiplying by 100% makes it a percentage error. \\[\\begin{equation} {\\displaystyle \\operatorname {MSE} ={\\frac {1}{n}}\\sum _{i=1}^{n}(Y_{i}-{\\hat {Y_{i}}})^{2}} \\tag{1.3} \\end{equation}\\] That is, MSE (1.3) is the mean of \\({\\displaystyle \\left({\\frac {1}{n}}\\sum _{i=1}^{n}\\right)}\\) of the squares of the errors \\({\\displaystyle (Y_{i}-{\\hat {Y_{i}}})^{2}}\\) \\[\\begin{equation} \\operatorname{RMSE} = \\sqrt{MSE} \\tag{1.4} \\end{equation}\\] The RMSE (1.4) represents the square root of the differences between predicted values and observed values. These deviations are called residuals when the calculations are performed over the data sample that was used for estimation of prediction errors when computed out-of-sample. 1.3.2 Forecasting Models of Spatial Network Data Forecasting models of network data have traditionally focussed on time and flow series approaches. In such approaches, the network structure is ignored and each of the time series at the nodes is forecast independently. Because of the many factors that determine urban traffic, it is very difficult to model traffic as a physical process. The majority of the literature on traffic forecasting focuses on data-driven approaches. In such methods, statistical relationships are extracted from historical data, and used to forecast future traffic conditions. Observations of a space-time process are made at the nodes of a spatial network, which are sampling locations such as weather stations or traffic flow detectors. At each node an individual time series is collected that describes the temporal evolution of the space-time process at a single point in space. A time series \\(z=\\{z(t)/z \\in T\\}\\) in temporal domain T is a sequence of observations of a process taken at equally spaced time intervals: \\(z_t, z_{t-1}, z_{t-2}, … z_{t-{(n-1)}}\\), where \\(z_t\\) denotes the value of some quantity \\(z\\) at time \\(t\\) and \\(n\\) is the lengh of the serie. \\(z_{t-{(n-1)}}\\) indicates the time at which the first available observation was made (for further details, see Haworth 2013). 1.4 Forecasting Data and Methods To model time series data we need to understand past behavior to project future values. We need to explore datasets characteristics in order to examine their attributes and evaluate the methods that would be applied. 1.4.1 Data Description The traffic data used in this book are travel time times (TTs) provided by Transport for London (TfL) as part of the London Congestion Analysis Project (LCAP). To ensure accurate results with exploratory data analysis and predictive models, some links have been removed from the original dataset due to poor data quality4. To visualize the The London Congestion Analysis Project Road Links we need to install the package tmap. Note that any package need to be installed first using the command install.packages('package_name') before it can be loaded using the command library(). Before a library is loaded its functions will be inaccessible, even if it is installed. # install if necessary using the command &#39;install.packages&#39; library(tmap) The package tmap (thematic map) creates a tmap-element that specifies a spatial data object, which we refer to as shape. tmap is a library that simplifies many of the elements of map creation. It uses the basic syntax style of the most popular graphics library in R. # first, we need to load the data to the space environement load(&quot;forecasting.RData&quot;) # the command &#39;tm_shape&#39; is used to setup and display the map bmap &lt;- read_osm(LCAPShp, type=&quot;osm&quot;) tm_shape(LCAPShp) + tm_lines(col=&quot;LENGTH&quot;, style=&quot;jenks&quot;, palette=&quot;Dark2&quot;, lwd=2.5)+ tm_shape(bmap)+tm_rgb(alpha = 0.3)+ tm_compass(position = c(&quot;left&quot;, &quot;top&quot;)) + tm_scale_bar(position = c(&quot;right&quot;, &quot;bottom&quot;))+ tm_layout(main.title=&quot;LCAP: subset of road links&quot;, frame.lwd = 4, inner.margins = 0.01, legend.position = c(&quot;left&quot;, &quot;bottom&quot;), legend.bg.color = &quot;white&quot;, legend.frame = &quot;black&quot;, main.title.color = &quot;#382d2d&quot;, compass.type = &quot;8star&quot;, main.title.position = c(&quot;center&quot;, &quot;top&quot;)) Figure 1.4: The London Congestion Analysis Project Road Links. LCAP is a system of automatic number plate recognition (ANPR) cameras, maintained by TfL, that monitors London’s road network 24 hours a day. Link travel times of individual vehicles are recorded and aggregated at the 5 minute level, providing travel time information on all of London’s major roads with 288 observations per link per day.Figure 1.5 displays the main features of the monitor system. Figure 1.5: Travel Time Data (source: adapted from Haworth 2013). The TT of a vehicle can be defined as the time taken to travel between two fixed points, for example, between one intersection and the next, or between home and work. Formally, the TT of an individual vehicle can be expressed as: \\[\\begin{equation} \\operatorname{TT} = t_2 - t_1 \\tag{1.5} \\end{equation}\\] Where \\(t_1\\) is the time at which the vehicle began its journey and \\(t_2\\) is the time at which it completed its journey. The road networks of major cities are equipped with sensor networks that measure traffic properties directly from traffic stream. The data used in the turotial part of this book were collected using automatic number plate recognition (ANPR) technology on London’s road. Each road link is paired with two ANPR cameras installed on the start and the end of the link. The first camera records the vehicle license plate and entrance time and the second camera records the exit time by matching the license plate. For this study individual TTs were aggregated at 5 minutes intervals giving 180 observations per day between 6am and 9pm. The travel time data is presented in unit travel times (second/metre). 1.4.2 Applied Methodologies Quantitative forecasting can be applied when two conditions are satisfied: numerical information about the past is available; it is reasonable to assume that some aspects of the past patterns will continue into the future. There is a wide range of quantitative forecasting methods, often developed within specific disciplines for specific purposes. Each method has its own properties, accuracies, and costs that must be considered when choosing a specific method. When forecasting time series data, the aim is to estimate how the sequence of observations will continue into the future. To evaluate forecast accuracy it is common practice to separate the available data into two portions, training and test data. The training data is applied to estimate the parameters of the method and the test data is used to evaluate its accuracy. Figure 1.6: Training Data. The literature on space-time forecasting can be broadly separated into two categories: the statistical parametric framework and machine learninhg models. Statistical parametric framework models are usually based on some form of autoregression (AR), whereby the current value of a series is modelled as a function of its previous values. The space-time autoregressive integrated moving average (STARIMA) is a widelly used parametric modelling framework for spatio-temporal data. A parametric model is one where the functional form of the relationship between the dependent and independent variables is described by a set of parameter and for stationary time series in which there is no significant trend in the data over time. However, it is not unusual for time series data to exhibit some type of upward or downward trend over time, like trend and seasonability. Trend: is the sweep or general direction of movement in a time series. It reflects the net influence of long-term factors that affect the time series in a consistent and gradual way over time (Hyndman and Athanasopoulos 2018). Seasonality: a regular, repeating pattern in the data or the presence of variations that occur at specific regular intervals less than a year. For example, urban traffic usually shows the same patterns on working days: heavy traffic are expected in the morning and after working hours. As more real world datasets become available at higher spatial and temporal resolutions, the task of describing space-time structures with conventional methods becomes more difficult. Non-parametrics methods as Machine Learning provides an alternative approach. ML models tend to be more complex black–box models that are created to maximize predictive accuracy. They are used to model complex, nonlinear, non-stationary relationships in massive, real world datasets. Machine learning uses past data to make accurate predictions of future events. Learning refers to the iterative process of finding a classifier or regression algorithm with optimal solution by means of training data, and then to test simulation data under the same parameters. The general learning task is usually to find a mapping function between a limited set of input/output pairs \\({(x_1, y_1, ..., x_n, y_n)}\\), referred to as training data, where \\(n\\) is the number of training examples. This process is know as supervised learning. There are other strategies of learning, the principals can be described as follow: Supervised learning: given a set of labelled training data, a function is sought to predict labels for unseen data. Examples: Regression (e.g. kernel ridge regression, SVR, ANNs); classification (e.g. SVMs, random forests, ANNs). Semi-supervised learning: small amount of labelled data, large amount of unlabelled data. Using both can improve predictions. Examples: Generative models, low density separation, graphical methods. Unsupervised Learning: no labels are supplied with training data, and structure is sought. Examples: clustering (e.g. k-means, hierarchical), dimensionality reduction (e.g. PCA), self-organizing maps, density estimation, data mining techniques. Reinforcement learning: a computer must perform a task in a given environment without knowledge of whether it has come close to its goal. Examples: driverless cars, playing chess/Go against an opponent, DeepMind. (source: Cheng and Haworth 2019) The focus of ML is on generalisation, the ability of the algorithm to predict unseen data samples. It stands as a different approach of classical statistics, where the focus is on in-sample performance and different to data mining which seeks to discover new knowledge from unlabelled data. ML does not assume that the functional form of the relationship between dependent and independent variables is known and can be described by model parameter. The technique minimizes error on unseen data, known as structural risk minimization, with no prior assumptions made on the probability distribution of the data. The choice of method depends on the characteristics of the dataset. Modelling and forecasting of highly nonlinear, nonstationary and heterogeneous processes requires models that are able to account for these properties. Table 1.3 and 1.4 outline the mains characteristics of each methology. Table 1.3: Statistical Parametric Framework Models. Technique Main features The space-time autoregressive integrated moving average (STARIMA) STARIMA has been adapted from ARIMA models. Is the only one that explicitly accounts for spatial structure in the data through the use of a spatial weight matrix. Seasonal and Trend Decomposition using Loess Forecasting Model (STLF) The STLF model comprises of a time series decomposition and a non-seasonal forecasting method. It decomposes the time series into a seasonal component, a trend-cycle or simply trend component and the reminder component. Table 1.4: Machine Learning Models. Technique Main features Support Vector Machine (SVM) Makes use of the kernel trick to map the input data into a high dimensional feature space, where a linear algorithm is used to find a solution that becomes. Kernel ridge regression (KRR) Is a related Kernel method based on linear least squares regression. Artificial Neural Networks (ANN) The basic structure of an ANN is a layered, feed-forward, completely connected network of artificial neurons, termed nodes. Machine learning uses past data to make accurate predictions of future events. Learning5 refers to the iterative process of finding a classifier or regression algorithm with optimal solution by means of training data, and then to test simulation data under the same parameters. The focus of ML is on generalisation, the ability of the algorithm to predict unseen data samples. It stands as a different approach of classical statistics, where the focus is on in-sample performance and different to data mining which seeks to discover new knowledge from unlabelled data. ML does not assume that the functional form of the relationship between dependent and independent variables is known and can be described by model parameter.The technique minimizes error on unseen data, known as structural risk minimization, with no prior assumptions made on the probability distribution of the data 1.5 Chapter Summary In this chapter, you have been introduced to modelling networks and forecast methods and principles. Forecasting models of network data have traditionally focussed on time series approaches. In such approaches, the network structure is ignored and each of the time series at the nodes is forecast independently. Spatial network data combines the challenges of modelling time series and spatial series, which are autocorrelation, nonstationarity, heterogeneity and nonlinearity. Many methods have been developed to model time series data. A common approach involves trying several modeling techniques on a given data set and evaluating how well they explain the past behavior of the time series variable. The most used accuracy measures are the mean absolute deviation (MAD), the mean absolute percent error (MAPE), the mean square error (MSE), ant the root mean square error (RMSE). The literature on space-time forecasting can be broadly separated into two categories: the statistical parametric framework and machine learninhg models. The choice of method depends on the characteristics of the dataset. Modelling and forecasting of highly nonlinear, nonstationary and heterogeneous processes requires models that are able to account for these properties. References "],
["exploratory-spatio-temporal-data-analysis-and-visualisation.html", "2 Exploratory Spatio-temporal Data Analysis and Visualisation 2.1 Introduction 2.2 Examining Data Patterns and Characteristics 2.3 Spatio-temporal Dependence and Autocorrelation 2.4 Chapter Summary", " 2 Exploratory Spatio-temporal Data Analysis and Visualisation 2.1 Introduction In this section, we will introduce the tools and techniques for exploratory spatial-temporal data analysis (ESTDA). Exploratory data analysis (EDA) aims to explore datasets characteristics and generate hypotheses by means of graphical methods such as histograms, box-plots and scatterplots. To apply these techniques, we are going to explore the characteristics of selected 28 road links of the London Congestion Analysis Project (LCAP). ESTDA is the extension of EDA to spatio-temporal data. Time-series analysis and spatial data analysis methods are used separately to examine whether the data are correlated and stationary in time and space. The goal is to develop an understanding of the data by revealing key relationships and processes. Acording to Cheng and Haworth (2019), among the objectives of ESTDA, are included: maximising insight into a dataset; uncovering underlying structure; extracting important variables; detecting outliers and anomalies; testing underlying assumptions; developing parsimonious models (simplest model that can achieve a certain performance level). In this chapter, we will be applying a data-driven approach by making a list of data components and then transforming them into graphics. To explore the data, a methodological pathway must be taken. First by tidying the data, which means to store in a regular form in accordance with the semantics of commands be applied. Once the data is tidied, a common step is to transform the data. Transformation includes narrowing in on observations of interest, eventually creating new variables and calculating a summary statistics. And finally, visualisation techniques is also a common approach to knowledge generation. A good visualisation answer or even raise new questions about the data. 2.2 Examining Data Patterns and Characteristics To examine how data vary in space and time, It is important to explore their general characteristics. The observation of a variable or index at two or more locations over time comes to form a space-time series. The first step we need to take prior to any comand is to load the data to the space environement and to install some required packages. An R package is a collection of functions, data and documentations that extends the capabilities of base R. To install an R package, you might use a single line of code: install.packages (&quot;package&quot;) You might type that line of code in the console and then press ENTER to run it. R will download the packages from CRAN and install them onto your computer. Once the package is installed, you can load it with library() function. The basic information about a package is provided in the description file, where you can find out what the package does, who the author is, what version the documentation belongs to, the date, the type of license its use, and the package dependencies. You can also access the description file inside R with the command packageDescription(&quot;package&quot;), via the documentation of the package help(package = &quot;package&quot;), or online in the repository of the package. # set working directory accordingly with setwd() # install if necessary using &#39;install.packages&#39; library(readxl) library(rgdal) library(lattice) library(dplyr) library(tmap) library(tmaptools) library(readxl) library(knitr) # the package starima is not available on rcran # you must install directly from the working directory using the command &#39;install.packages&#39; install.packages(&quot;starima_package.R&quot;) # load the data to the space environement load(&quot;forecasting.RData&quot;) tts &lt;- read_excel(&quot;tts.xlsx&quot;, sheet = &quot;transpose&quot;, col_names = TRUE) To introduce the fundamentals of ESTDA, we will explore the technique to 28 road links selected within the London Congestion Charging Zone (CCZ).6 Within this area, a fee is charged on most motor vehicles operating in Central London between 07:00 and 18:00 Mondays to Fridays. It is not charged on weekends, public holidays or between Christmas Day and New Year’s Day. An example of road link shape is presented in figure 2.1. Figure 2.1: Road Link ID 2061. We then need to select the road links IDs from the data matrix UJT. UJT.chosen &lt;- UJT[ ,c(&quot;1745&quot;,&quot;1882&quot;,&quot;1622&quot;,&quot;1412&quot;,&quot;1413&quot;,&quot;1419&quot;,&quot;2090&quot;,&quot;2054&quot;,&quot;2112&quot;, &quot;2357&quot;,&quot;2364&quot;,&quot;2059&quot;,&quot;2061&quot;,&quot;2062&quot;,&quot;2358&quot;,&quot;2361&quot;,&quot;2363&quot;,&quot;2084&quot;, &quot;2086&quot;,&quot;2097&quot;,&quot;2318&quot;,&quot;2344&quot;,&quot;2087&quot;,&quot;2088&quot;,&quot;2089&quot;,&quot;2282&quot;,&quot;2102&quot;,&quot;473&quot;)] Also, we need to clean shapefile LCAPAdj to the chosen road links. LCAPShp.chosen &lt;- LCAPShp[LCAPShp$LCAP_ID %in% c(&quot;473&quot;,&quot;1622&quot;,&quot;2344&quot;,&quot;1745&quot;,&quot;2090&quot;,&quot;2054&quot;,&quot;2112&quot;,&quot;2357&quot;,&quot;2364&quot;, &quot;2059&quot;,&quot;2061&quot;,&quot;2062&quot;,&quot;2358&quot;,&quot;2361&quot;,&quot;2363&quot;,&quot;2084&quot;,&quot;2086&quot;,&quot;2097&quot;, &quot;2318&quot;,&quot;2087&quot;,&quot;2088&quot;,&quot;2089&quot;,&quot;2282&quot;,&quot;2102&quot;,&quot;1882&quot;,&quot;1412&quot;,&quot;1413&quot;,&quot;1419&quot;), ] To display the subset of the network roads extracted from the LCAP network, we can use the code tm_shape() to specify the dataset to map. The second part, tm_lines() specifies that we want to plot line data. We can also add additional elements and change the way elements are displayed using the + operator followed by a function. tmap_mode(mode =&quot;view&quot;) ## tmap mode set to interactive viewing tm_shape(LCAPShp.chosen) + tm_lines(col=&quot;LCAP_ID&quot;, style=&quot;cat&quot;, palette=&quot;Accent&quot;, lwd=3.5)+ tm_layout(legend.bg.color = &quot;white&quot;, legend.frame = &quot;black&quot;) ## Linking to GEOS 3.6.1, GDAL 2.2.3, PROJ 4.9.3 2.2.1 Non Spatio-temporal Data Patterns Before we examine how the data vary in space and time, we should explore their general characteristics. A very simple first step is to start by calculating the mean and standard deviation: mu = mean(UJT.chosen) mu ## [1] 0.2109794 sdev = sd(UJT.chosen) sdev ## [1] 0.139087 Next, a histogram of the length of the selected road links can be calculated and plotted on a histogram (figure 2.2). A histogram divides the data into groups of equal size and plots the number of observations (frequency) occurring in each group on the y-axis. The mean as a vertical line is also displayed. The distribution of the data shows that road links present a wide range of length, falling in the range from 1000 to 2000 metres. # setup a histogram road link length hist(LCAPShp.chosen@data$LENGTH, main=&quot;Length of Road Links Histogram&quot;, xlab=&quot;metres&quot;, col=&quot;yellow&quot;) abline(v=mean(LCAPShp.chosen@data$LENGTH), col=&quot;red&quot;) Figure 2.2: Histogram of link lengths and mean value shown in red line. Through our simple aspatial exploratory data analysis we have discovered that UTTs are not normally distributed. Some initial hypotheses might be drawn. Are the distribution of UTTs been affected by heavy traffic flow? or it is a simple correlation to the lenght of the roads? Further investigations must be carried out. To examine urban travel patterns, the unit of travel time (UTT) is a commonly applied index since it normalizes the TTs with the length of each road link. It is a well know index to study traffic flow. A boxplot in figure 2.3 illustrates the UTTs distribution across the 28 links for a period of 30 days. The majority (for example road ID 2087, 2088 and 2112) presents low variation which suggests smoother traffic conditions, whereas a few links (e.g. 473, 2061 and 2062) exhibit high variation which might be related to traffic congestion. # The next step is to examine the distribution of the data. # Boxplot(UJT.chosen) boxplot(UJT.chosen, main=&quot;Aggregated UTT Boxplot&quot;, xlab=&quot;Road Link ID&quot;, ylab=&quot;seconds/metre&quot;, col=&quot;light blue&quot;) Figure 2.3: Boxplot with UTT within a 30 days period by road link ID. The nature of urban road traffic is quite different from traffic on highways because of traffic signals at intersections. The speed of the traffic stream on urban roads is determined by the interaction between the demand for road space and the supply of road space, known as capacity (Bell and Y.Iida 1997). Congestion is characterised by slow moving stop and go traffic. Congestion that results purely from the level of demand outstripping the capacity of the road way is referred to as recurrent congestion. Recurrent congestion occurs on a daily basis and is determined by commuting patterns(Haworth 2013). The level of demand also varies between weekdays and weekends, resulting in different traffic patterns on different days of the week. To investigate the distribution of the data, histograms of the UTT (2.4 and 2.5) can be created wtih the function hist() Note that you can define the visual property of the graph (for example the color, the size) with aesthetic property definitions. An aesthetic is a visual property of the objects on your graph. it becomes clear that the data are not normally-distributed: most of the UTT lay between 0 and 0.5, and has a mean of 0.21 sec/m. The same histogram displays the upper end frequency and shows the index laying well above the mean at the tail. # Histogram of the unit travel time with lower end frequency. # Upper end frequency hist(UJT.chosen, main=&quot;Aggregated UTT Histogram&quot;, xlab=&quot;seconds/metre&quot;, xlim=c(3, 10.0), ylim=c(0,15.00), col=&quot;yellow&quot;) Figure 2.4: Histogram of the unit travel time with lower end frequency of UTT. # Histogram of the unit travel time with upper end frequency. # Lower end frequency hist(UJT.chosen, main=&quot;Aggregated UTT Histogram&quot;, xlab=&quot;seconds/metre&quot;, xlim=c(0, 4.0), ylim=c(0,170000), col=&quot;yellow&quot;) Figure 2.5: Histogram of the unit travel time with upper end frequency of UTT. Using a quantile-quantile (QQ) plot it is possible to better study how the distribution diverges from normality (2.6. The straight red line is the theoretical normal distribution, (i.e. if the data were normally distributed, they would fall on this line). It can be seen that the distribution diverges from normal at its upper tail. This means that the UTT of some road links present much higher indices than the average. Probably, these roads lay in congestion areas with slow moving stop and go traffic. # Using a quantile-quantile (QQ) plot we can examine how the distribution diverges from normality qqnorm(UJT.chosen) qqline(UJT.chosen, col=&quot;red&quot;) # The straight red line is the theoretical normal distribution Figure 2.6: QQ-Plot of UTTs in chosen road-links. Any hypotheses of UTTs behavior so far must be analysed by looking at how the data vary in space and time. As a simple first step, matrix of scatterplots showing the relationship can be drawn as shown on ??. A scatterplot matrix is a collection of scatterplots organized into a grid. Each scatterplot shows the relationship between a pair of variables, in this case between mean UTT, road link length and time. A conclusion can be reach with the scatterplot: there is a negative correlation between length and UTT. The unit of travel time is not defined by the length of the road. This is a common feature of urban traffic where heavy traffic leads to overcrowded roads with congestions and slow-moving stop and go traffic. There also seems to be some relationship between altitude and temperature, but it is difficult to see. We can explore this further by plotting mean temperature, altitude and latitude together on a 3D plot. Two ways of doing this using different R libraries are shown in figures 1.4 and 1.5. Another way to explore this relationship further is by plotting mean UTT, length and time on a 3D plot. We can explore this further by plotting mean UTT, lenght and travel time together on a 3D plot. Two ways of doing this using different R libraries are shown in figures 2.7 and 2.8 and ??. The graphs ratify the principal conclusion already draw: the unit of travel time is negatively correlated with the length of the road, which imply heavy traffic as the main cause of higher travel time. # Time scatterplot3d(x=TIME, y=LCAPShp.chosen@data$LENGTH, z=MeanUTT, xlab=&quot;TTime [sec]&quot;, ylab = &quot;Lenght[m]&quot;, zlab = &quot;Mean UTT [sec/m]&quot;) Figure 2.7: Scatterplots 3d with time on x axis # Lengh scatterplot3d(x=LCAPShp.chosen@data$LENGTH, y=TIME, z=MeanUTT, xlab=&quot;Lenght[m]&quot;, ylab = &quot;TTime [sec]&quot;, zlab = &quot;Mean UTT [sec/m]&quot;) Figure 2.8: Scatterplots 3d with lenght on x axis 2.2.2 Temporal Patterns The previous section provides a description of the date and introduces UTT as an index of traffic flow. It also shows that length of road link and UTT are not positively associated and that length and time are positively correlated. The daily patter of road links needs to be analysed next. First load the following packages: # install if necessary library(scatterplot3d) library(plot3D) library(rgl) library(plotly) ## Loading required package: ggplot2 ## ## Attaching package: &#39;plotly&#39; ## The following object is masked from &#39;package:ggplot2&#39;: ## ## last_plot ## The following object is masked from &#39;package:stats&#39;: ## ## filter ## The following object is masked from &#39;package:graphics&#39;: ## ## layout library(starma) A line plot showing the daily UTT mean over a period of 30 days can be set up using the package Plotly for R. Plotly is an package for creating interactive web-based graphs.7 Here we have selected four road links to desmonstrate how you can set up the graph. Even though some road links present similar patter, no clear conclusion can be drawn from the figure. lcap &lt;- read_excel(&quot;tts.xlsx&quot;, sheet = &quot;tts&quot;, col_types = c(&quot;numeric&quot;, &quot;numeric&quot;, &quot;numeric&quot;, &quot;numeric&quot;, &quot;numeric&quot;, &quot;numeric&quot;, &quot;numeric&quot;, &quot;numeric&quot;, &quot;numeric&quot;, &quot;numeric&quot;, &quot;numeric&quot;, &quot;numeric&quot;, &quot;numeric&quot;, &quot;numeric&quot;, &quot;numeric&quot;, &quot;numeric&quot;, &quot;numeric&quot;, &quot;numeric&quot;, &quot;numeric&quot;, &quot;numeric&quot;, &quot;numeric&quot;, &quot;numeric&quot;, &quot;numeric&quot;, &quot;numeric&quot;, &quot;numeric&quot;, &quot;numeric&quot;, &quot;numeric&quot;, &quot;numeric&quot;, &quot;numeric&quot;, &quot;numeric&quot;, &quot;date&quot;)) f &lt;- list( family = &quot;Courier New, monospace&quot;, size = 18, color = &quot;#7f7f7f&quot; ) x &lt;- list( title = &quot;Day&quot;, titlefont = f ) y &lt;- list( title = &quot;Mean UTT [seconds/metre]&quot;, titlefont = f ) lcap1h= aggregate(lcap, list(cut(lcap$datetime, &quot;1 day&quot;)), mean) p=plot_ly(x=lcap1h$Group.1, y=lcap1h$`473`,mode = &#39;lines&#39;, type=&quot;scatter&quot;, name=&quot;LCAP 473&quot;) %&gt;% layout(legend = f, xaxis =x, yaxis=y) p &lt;- add_trace(p,y=lcap1h$`1622`, name=&quot;LCAP 1622&quot;) p &lt;- add_trace(p,y=lcap1h$`2344`, name=&quot;LCAP 2344&quot;) p &lt;- add_trace(p,y=lcap1h$`1745`, name=&quot;LCAP 1745&quot;) p Figure 2.9: Time series of Mean UTT per day over 30 days. 2.3 Spatio-temporal Dependence and Autocorrelation When performing spatial analysis, we need to undestand the level of spatial dependence in the area of interest we are studying. Spatial dependence relates to the level in influence of nearby locations values to a particular location or area of interest. 2.3.1 Temporal autocorrelation In order to measure autocorrelation in temporal and spatial data, we use lagged variables. Spatial autocorrelation is the correlation of a time series with itself, separated by a temporal lag. In time series analysis, spatial dependence is usually measured by means of the autocorrelation funcion (ACF), calculated for a set of number lags and displayed on an ACF plot. A temporal lag is the interval of time between two related phenomena (such as a cause and its effect). Temporal Autocorrelation is the correlation of a time series with itself, separated by a temporal lag. Temporal autocorrelation quantifies the extent to which near observations of a process are more similar than distant observations in time. A value of 1 indicates perfect positive autocorrelation, -1 perfect negative autocorrelation, and 0 no autocorrelation. The plot on figure 2.10 displays the level of correlation of the time series with itself at each of the first 1260 lags (7 days of data) for a single road link (RoadID: 2112) The blue line is approximate 95% confidence interval. The slow decrease in the ACF as the lags increase is due to the trend, while the scalloped shape is due the seasonality. acf(UJT.chosen[,&quot;2112&quot;], lag.max = 1260, main= &quot;ACF Roadlink 2112&quot;, plot = TRUE) Figure 2.10: Temporal autocorrelation function of UTT at 1260 lags for road link 2112. In order to visualise the time series of all selected road links in a single view, we can use a heatmap as a visualisation method that can be used to look for patterns in a dataset. It allows relatively large datasets or correlation matrices to be visualised in a single view. We need data in matrix format as an input of the heatmap function. Figure 2.11 presents a heatmap of autocorrelation at 180 lags, a single day of UTT data (x-axis), for the 28 selected road links of the study (y-axis). We can come to the evidence of a daily pattern of autocorrelation that could be explained by the travel times during peak hours, a common phenomenon during working days. # Create the heatmap ac &lt;- acf(UJT.chosen, lag.max = 180, plot = FALSE) acm &lt;- t(ac$acf[,1,1]) for(i in 2:28){ acm &lt;- rbind(acm, t(ac$acf[,i,i])) } rownames(acm) &lt;- colnames(UJT.chosen) levelplot(t(acm), aspect=&quot;fill&quot;, xlab=&quot;TEMPORAL LAG&quot;,ylab=&quot;ROAD LINK ID&quot;) Figure 2.11: Temporal autocorrelation heatmap of selected road links: daily UTT in 01-01-2011. A space-time autocorrelation function must also be carried out to analyse the spatio-temporal relationship between road links on the network. To perform the analysis, a spatial weight matrix must be set-up. A spatial weights matrix is a representation of the spatial structure of the data. It is a quantification of the spatial relationships that exist among the features in your dataset. Conceptually, the spatial weights matrix is an \\(N*N\\) table (N is the number of features in the dataset). There is one row for every feature and one column for every feature. The cell value for any given row/column combination is the weight that quantifies the spatial relationship between those row and column features. There are two strategies for creating weights to quantify the relationships among data features: binary or variable weighting. For binary strategies (e.g. fixed distance, K nearest neighbors, Delaunay Triangulation, contiguity, or space-time window) a feature is either a neighbor (1) or it is not (0). For weighted strategies (inverse distance or zone of indifference), neighboring features have a varying amount of impact (or influence), and weights are computed to reflect that variation. Examples of non-binary weighting schemes include length of shared border between counties, proportion of shared perimeter or number of neighbours. Spatial weight matrices are created in R using the spdep package. To construt the spatial weight matrix, we need first the adjacency matrix. A common procedure to calculate the spatial weight matrix is to calculate the distance between two adjacent links by their lengths. LCAPAdj.chosen &lt;- LCAPAdj[c(&quot;2097&quot;,&quot;2318&quot;,&quot;2344&quot;,&quot;2087&quot;,&quot;2088&quot;,&quot;2089&quot;,&quot;2282&quot;,&quot;2102&quot;,&quot;473&quot;) , c(&quot;2097&quot;,&quot;2318&quot;,&quot;2344&quot;,&quot;2087&quot;,&quot;2088&quot;,&quot;2089&quot;,&quot;2282&quot;,&quot;2102&quot;,&quot;473&quot;)] knitr::kable(LCAPAdj.chosen[]) 2097 2318 2344 2087 2088 2089 2282 2102 473 2097 0 0 0 1 0 0 0 0 0 2318 0 0 1 0 0 0 0 0 0 2344 0 0 0 0 0 0 0 0 0 2087 0 0 0 0 0 0 0 0 0 2088 0 0 0 0 0 0 0 0 0 2089 0 0 0 0 0 0 0 0 0 2282 0 0 0 0 0 0 0 0 0 2102 0 0 0 0 0 0 0 0 0 473 0 0 0 0 0 0 0 0 0 ws &lt;- read_excel(&quot;tts.xlsx&quot;, sheet = &quot;Sheet3&quot;, range = &quot;B1:AC29&quot;) wm &lt;- data.matrix(LCAPAdj.chosen * ws) knitr::kable(wm[15:28,15:28]) 2358 2361 2363 2084 2086 2097 2318 2344 2087 2088 2089 2282 2102 473 0 0 0 0 0 0 0.000 0 0 0.000 0 0 0.000 0 0 0 0 0 0 0 0.000 0 0 0.000 0 0 0.000 0 0 0 0 0 0 0 0.000 0 0 0.000 0 0 0.000 0 0 0 0 0 0 0 0.000 0 0 0.000 0 0 0.000 0 0 0 0 0 0 0 0.000 0 0 0.000 0 0 0.000 0 0 0 0 0 0 0 0.000 0 0 0.000 0 0 0.000 0 0 0 0 0 0 0 0.000 0 0 0.000 0 0 2115.902 0 0 0 0 0 0 0 0.000 0 0 0.000 0 0 0.000 0 0 0 0 0 0 0 0.000 0 0 0.000 0 0 0.000 0 0 0 0 0 0 0 0.000 0 0 1914.014 0 0 0.000 0 0 0 0 0 0 0 0.000 0 0 0.000 0 0 0.000 0 0 0 0 0 0 0 0.000 0 0 0.000 0 0 0.000 0 0 0 0 0 0 0 2115.902 0 0 0.000 0 0 0.000 0 0 0 0 0 0 0 0.000 0 0 0.000 0 0 0.000 0 To support determining if the data is stationary and the type of correlation, lag plots might be calculated for the first four lags of each raod link. Figure 2.12 presents the plot for raodlink 473. We can see that there is an increasing scatter with progressive lags and due to the clustering around the slopes in segments 473. Furthermore, it can be state that temporal interdependence between consecutive observations is linear. lag.plot(UJT.chosen[,&quot;2344&quot;], lags = 10, main= &quot;ACF, 2344&quot;, do.lines = FALSE) Figure 2.12: First ten lag plots for road link ID 2344. 2.4 Chapter Summary Exploratory spatial data analysis (ESDA) is the extension of exploratory data analysis (EDA) to the problem of detecting spatial properties of data sets where, for each attribute value, there is a locational datum. EDA is a collection of descriptive techniques for detecting patterns in data, identifying unusual or interesting features, distinguishing accidental from important features and for formulating hypotheses from data. The set of exploratory techniques combines techniques that are visual (including charts, graphs and ®gures) and numerical but statistically robust. Among the many tools that enable the analysis of spatial relationships, are (auto)correlations, densities and clustering patterns. ESDA is an extension of EDA to detect spatial properties of data: to detect spatial patterns in data, to formulate hypotheses which are based on, or which are about, the geography of the data and to assess spatial models. References "],
["seasonal-and-trend-decomposition-with-loess-forecasting-model-stlf.html", "3 Seasonal and Trend Decomposition with Loess Forecasting Model (STLF) 3.1 Introduction 3.2 Method Description 3.3 The Modeling and Solving Approach 3.4 Chapter Summary", " 3 Seasonal and Trend Decomposition with Loess Forecasting Model (STLF) 3.1 Introduction In Chapter 2, we see that parametric models describe the functional form of the relationship between the dependent and independent variables. It is not unusual for time series data to exhibit some type of upward or downward tendency over time, like trend and seasonability. Time series comprise three eseential components: 1. a trend-cycle component, 2. a seasonal component and 3. a remainder component (containing anything else in the time series). When we decompose a time series into components, we usually combine the trend and cycle into a single trend-cycle component. In this chapter, we will consider some common methods for extracting these components from a time series. Often this is done to help improve understanding of the time series, but it might also be used to improve forecast accuracy. It is important to mention that given the weak spatio-temporal autocorrelation (previoulsly examined in chapter 2) and the strong temporal autocorrelation on the LCAP network, the use of a local regression model does not require the series to be stationary (in contrast to ARIMA/Box-Jenkins approaches). The STL model is applied to time series decomposition with a non-seasonal forecasting method and was developed by Cleveland (Cleveland and Terpenning 1990). In this chapter, we will apply the method to the data previously examined and we will select some road links to carry-on the method. 3.2 Method Description The seasonal-trend decomposition with Loess (STLF) is a method for decomposing time series into: a seasonal component \\(S_t\\), a trend-cycle or simply trend component \\(T_t\\), and the reminder or error component \\(R_t\\) (Cleveland and Terpenning 1990). This decomposition may be additive (3.1) or multiplicative (3.2) where \\(y_t\\) is the data. A seasonally adjusted series is the result of removing the seasonal component from the data. \\[\\begin{equation} \\operatorname {y_t} = S_t + T_t + R_t \\tag{3.1} \\end{equation}\\] \\[\\begin{equation} \\operatorname {y_t} = S_t \\times T_t \\times R_t \\tag{3.2} \\end{equation}\\] The method known as exponential smoothing state space model (ETS) is applied to algorithms which generate point forecasts to define prediction (or forecast) intervals. ETS is able to generate a complete forecast distribution from an univariate time series using exponentially decaying weighted averages of past observations. The state space models correspond to the equations that describe how the error, trend, and seasonal components vary over time. Hyndman and Athanasopoulos (2018) describe that each of them can be categorised as additive (A), multiplicative (M), or none (N). For example, ETS(A,N,N) represents a simple exponential smoothing with additive errors method and ETS(M,A,N) a Holt’s linear method with multiplicative errors (Holt 2004). Putting them together, the STLF model applies a decomposition to a time series. It then models the seasonally adjusted series with an ETS method, re- seasonalizes the result, and finally returns the predictions or forecasts. 3.3 The Modeling and Solving Approach To apply the model, we have divided the data as follows: the training set contains the UTT data for the first 23 days and the testing set considers the remaining 7 days. As previously described, there are 180 observations per day, one very 5 minutes from 6 am to 9pm. It is important to note that the model accounts only for temporal characteristics and no difference is going to be considered if the road links are adjacent on non-adjacent. To set up the experience, first load tha data into the space environement as well as the required packages. # set the working directory accordingly with setwd() # load the data to the space environement load(&quot;wksp2404.RData&quot;) ## Registered S3 method overwritten by &#39;xts&#39;: ## method from ## as.zoo.xts zoo ## Registered S3 method overwritten by &#39;quantmod&#39;: ## method from ## as.zoo.data.frame zoo ## Registered S3 methods overwritten by &#39;forecast&#39;: ## method from ## fitted.fracdiff fracdiff ## residuals.fracdiff fracdiff # install if necessary using &#39;install.packages&#39; library(grid) library(ggplot2) library(forecast) library(grid) In this experiment, four roads links have been randomly selected: ids. 2112, 2087, 2061 and 473. First, we will analyse the time series pattern of each road link. A daily pattern can be observed for road IDs 2112 and 2087 as displayed in Figure 3.1. We can also observe that the same pattern is not apparant for the first 2 roads because of large UTT values that can reach up to 8 seconds per metre. p1 &lt;- autoplot(ts_roadLink.2061, xlab =&quot;Days&quot;, ylab=&quot;UTT 2061&quot;) p1 &lt;- p1 + theme(text = element_text(size = 15)) p2 &lt;- autoplot(ts_roadLink.473, xlab =&quot;Days&quot;, ylab=&quot;UTT 473&quot;) p2 &lt;- p2 + theme(text = element_text(size = 15)) p3 &lt;- autoplot(ts_roadLink.2112, xlab =&quot;Days&quot;, ylab=&quot;UTT 2112&quot;) p3 &lt;- p3 + theme(text = element_text(size = 15)) p4 &lt;- autoplot(ts_roadLink.2087, xlab =&quot;Days&quot;, ylab=&quot;UTT 2087&quot;) p4 &lt;- p4 + theme(text = element_text(size = 15)) # plot the graphs grid.newpage() grid.draw(rbind(ggplotGrob(p1), ggplotGrob(p2), ggplotGrob(p3), ggplotGrob(p4), size = &quot;last&quot;)) Figure 3.1: Training data set: UTT time series (23 first days) of 4 links. A polar plot displays a plot of radial lines, symbols or a polygon centered at the midpoint of the plot frame on a 0:360 circle. Positions are interpreted as beginning at the right and moving counterclockwise unless start specifies another starting point or clockwise is TRUE. If add = TRUE is passed as one of the additional arguments, the values will be added to the current plot. Figure 3.2 presents polar charts of UTT for the first 3 days of road links 2061 and 473. A pattern can be observed between the different days (01/01/2011 - 03/01/2011), so a seasonal trend can be assumed for these road links as well. ts_seasonplot = ts(training[1:540,&quot;2061&quot;], start=0, frequency=180) ts_seasonplot2 = ts(training[1:540,&quot;473&quot;], start=0, frequency=180) p1 &lt;- ggseasonplot(ts_seasonplot, polar=TRUE) + ylab(&quot;seconds/metre&quot;) + xlab(&quot;Temporal lags&quot;) + theme(axis.title = element_text(size = 15)) + theme(legend.title=element_blank())+ ggtitle(&quot;Polar seasonal plot: Road 2061 UTT&quot;) p2 &lt;- ggseasonplot(ts_seasonplot2, polar=TRUE) + ylab(&quot;seconds/metre&quot;) + xlab(&quot;Temporal lags&quot;) + theme(axis.title = element_text(size = 15)) + theme(legend.title=element_blank())+ ggtitle(&quot;Polar seasonal plot: Road 473 UTT&quot;) grid.newpage() grid.draw(cbind(ggplotGrob(p1), ggplotGrob(p2), size = &quot;last&quot;)) Figure 3.2: Polar seasonal plots: UTT time series (3 first days) of links with higher variation. Next, the STL decomposition is applied for the 23 days time series. The STL() function of stats package decomposes the time series into seasonal, trend and irregular components using loess.8 Initially, the default values of the parameters are used, where the span in lags of the loess window for seasonal extraction, function s.window(), is periodic and for the trend extraction function t.window() is defined by (1.5*period) / (1-(1.5/s.window). Here, we will select two road links: 2112 and 473. ts_train_2112 &lt;- ts_roadLink.2112 ts_test_2112 &lt;- ts_test.2112 ts_train_473 &lt;- ts_roadLink.473 ts_test_473 &lt;- ts_test.473 ts_train_2112 %&gt;% stl(s.window = &quot;periodic&quot;) %&gt;% autoplot() + xlab(&quot;Day&quot;) + theme(text = element_text(size = 15)) + ggtitle(&quot;Seasonal Decomposition: Road Link 2112 UTT&quot;) Figure 3.3: STL decomposition for time series. ts_train_473 %&gt;% stl(s.window = &quot;periodic&quot;) %&gt;% autoplot() + xlab(&quot;Day&quot;) + theme(text = element_text(size = 15)) + ggtitle(&quot;Seasonal Decomposition: Road Link 473 UTT&quot;) Figure 3.4: STL decomposition for time series. The STLF() function of the forecast package in R runs the STL + ETS approach with the following default parameters: stlf(y, h = frequency(x) * 2, s.window = 13, t.window = NULL, robust = FALSE, lambda = NULL, biasadj = FALSE, x = y, ...)` Some additional parameters are included here: h(): number of lags or periods for forecasting. robust(): If TRUE, robust fitting will used in the loess procedure within STL. lambda(): box-Cox transformation parameter. There will be no transformation if it is equal to NULL. Otherwise, the forecasts are back-transformed via an inverse Box-Cox transformation (Hyndman and Athanasopoulos 2018). If set to auto, a transformation value is selected using the BoxCox.lambda() function. biasadj(): use adjusted back-transformed mean for Box-Cox transformations. If transformed data is used to produce forecasts and fitted values, a regular back transformation will result in median forecasts. If biasadj() is TRUE, an adjustment will be made to produce mean forecasts and fitted values.9 Additional parameters are specified in the documentation of the R package. For this study we will set the following parameters: h =1260 (7 days from 6 am to 9 pm) s.window = “periodic f_stl_2112 = stlf(ts_train_2112, h=1260, s.window = &quot;periodic&quot;, lambda = NULL) f_stl_473 = stlf(ts_train_473, h=1260, s.window = &quot;periodic&quot;) p1 &lt;- autoplot(f_stl_2112) + xlab(&quot;Day&quot;) + ylab(&quot;Road link 2112 UTT&quot;) p1 &lt;- p1 + theme(text = element_text(size = 15)) p2 &lt;- autoplot(f_stl_473) + xlab(&quot;Day&quot;) + ylab(&quot;Road link 473 UTT&quot;) p2 &lt;- p2 + theme(text = element_text(size = 15)) # plot the graphs grid.draw(rbind(ggplotGrob(p1), ggplotGrob(p2), size = &quot;last&quot;)) Figure 3.5: Initial STLF UTT prediction for the next 7 days. From figure 3.5, it can be observed that as the model predicts UTT values several steps ahead, uncertainty increases. Thus, the interpretation of the forecasts can be significantly misleading if this uncertainty in not accounted for. To be noticed as well, road link 473 presents additive error, whereas road link 2112 presents multiplicative errors. The following figure shows the diagnostic checking of the residuals for road link 2112 and 473. It can be observed that the residuals are normally distributed. However, some of the residuals are correlated meaning that there is more information in the errors that STLF has not accounted for. checkresiduals(f_stl_2112) ## Warning in checkresiduals(f_stl_2112): The fitted degrees of freedom is ## based on the model used for the seasonally adjusted data. Figure 3.6: Residuals from STLF model applied to the time series. ## ## Ljung-Box test ## ## data: Residuals from STL + ETS(M,N,N) ## Q* = 606.21, df = 358, p-value = 4.663e-15 ## ## Model df: 2. Total lags used: 360 # If the residuals are not normally distributed and uncorrelated then # there is more information in the errors that has not been accounted for in the model. # plot predictions vs obs predicted &lt;- f_stl_2112[[&quot;mean&quot;]] observed &lt;- ts_test_2112 checkresiduals(f_stl_473) ## Warning in checkresiduals(f_stl_473): The fitted degrees of freedom is ## based on the model used for the seasonally adjusted data. Figure 3.7: Residuals from STLF model applied to the time series. ## ## Ljung-Box test ## ## data: Residuals from STL + ETS(A,N,N) ## Q* = 480.28, df = 358, p-value = 1.606e-05 ## ## Model df: 2. Total lags used: 360 # If the residuals are not normally distributed and uncorrelated then # there is more information in the errors that has not been accounted for in the model. # plot predictions vs obs predicted &lt;- f_stl_473[[&quot;mean&quot;]] observed &lt;- ts_test_473 We can use the model for one-step-ahead prediction and plot the results, which are shown in figure 3.8. The graph shows the plots of the predicted UTT for road link 2112 and the observed or real values. predicted &lt;- f_stl_2112[[&quot;mean&quot;]] observed &lt;- ts_test_2112 # Simple plot plot(observed, col = &quot;blue&quot;, xlab = &quot;Days&quot;, ylab=&quot;UTT&quot;) lines(predicted, col=&quot;red&quot;,lty=2) legend(23,max(observed),legend=c(&quot;Observed&quot;,&quot;Predicted&quot;), col=c(&quot;blue&quot;,&quot;red&quot;),lty=c(1,2), ncol=1) Figure 3.8: Observed and predicted values of UTT for road link 2112. We can finally print the measurements of performance of our experiment as displayed in figure ??. accuracy(predicted, observed) ## ME RMSE MAE MPE MAPE ACF1 ## Test set 0.02958335 0.04571804 0.03563482 13.4773 18.05772 0.7540641 ## Theil&#39;s U ## Test set 1.660111 Where: ME: Mean Error RMSE: Root Mean Squared Error MAE: Mean Absolute Error MPE: Mean Percentage Error MAPE: Mean Absolute Percentage Error MASE: Mean Absolute Scaled Error The first 5 are traditional measurements of forecast accuracy or performance. MASE was introduced by Hyndman who suggested that it should become the standard metric for comparing forecast accuracy across multiple time series, where the forecast error is scaled by the in-sample mean absolute error obtained using the naïve forecasting method (Hyndman and Athanasopoulos 2018). MASE values greater than 1 show the forecasts are poorer than one-step forecasts from the naïve method10 3.4 Chapter Summary Time series decomposition involves thinking of a series as a combination of level, trend, seasonality, and noise components. Decomposition provides a useful abstract model for thinking about time series generally and for better understanding problems during time series analysis and forecasting. Seasonal and Trend Decomposition with Loess Forecasting Model is a versatile and robust method for decomposing time series. The model presents several advantages over other methods as SEATS and X11 decomposition. STLF can handle any type of seasonality, not only monthly and quarterly data. The seasonal component is allowed to change over time, and the rate of change can be controlled by the user. The smoothness of the trend-cycle can also be controlled by the user. It can be robust to outliers (i.e., the user can specify a robust decomposition), so that occasional unusual observations will not affect the estimates of the trend-cycle and seasonal components. They will, however, affect the remainder component. knitr::opts_chunk$set(error = TRUE) References "],
["support-vector-regression.html", "4 Support Vector Regression 4.1 Introduction 4.2 Method Description 4.3 The Modeling and Solving Approach 4.4 Chapter Summary", " 4 Support Vector Regression 4.1 Introduction In this section, the methodology Support Vector Regression (SVR) is applied for travel-time prediction. SVR is a computational technique that has its root on machine learning (ML) strategies. The feasibility of applying SVM in travel-time prediction is demonstrated in this section, the results and analysis are further presented. 4.2 Method Description SVR is an extension of ML technique known as support vector machine (SVM), to regression problems. SVMs make use of a hypothesis space of linear functions in a feature space, trained with a learning algorithm from optimisation theory. An important aspect of SVM is that not all the available training examples are used in the training algorithm. The subset of points that lie on the margin, called support vectors are the only ones that defines the hyperplane of maximin margin. SVMs techniques are grounded in the basic principle that for a given learning task, with a given finite number of training data, the best generalization performance will be achieved if the right balance is struck between the accuracy attained and the capacity of the machine to learn any training set without error (Burges 1998).11 Figure 4.1 presents the classic example of linear support vector machine. Figure 4.1: Linear support vector machine (source: Burges 1998). SVM algorithms use a set of kernel functions. The function of kernel is to take data as input and transform it into the required form. Kernel methods in ML are used in a different way comparing to spatial analysis (e.g. Kernel Density Estimation, KDE). Instead of being applied to spatial distances, they are applied to distances between sets of variables. Different SVM algorithms use different types of kernel functions, for example linear, nonlinear, polynomial, radial basis function (RBF), and sigmoid. Figure 4.2 presents the basic scheme of kernel function method applied to ML. Figure 4.2: Kernel to Machine Learning (source: Cheng and Haworth 2019). A kernel function is usually used to refer to the kernel trick, a method of using a linear classifier to solve a non-linear problem. It entails transforming linearly inseparable data like to linearly separable ones. Machine learning methods are widely applied to classification and regression problems. The best-known kernel method for regression is support vector regression (SVR), which is based on the principles of statistical learning theory (Cortes and Vapnik 1995). Kernel methods convert linear algorithms for use on nonlinear data by projecting the input data into a high dimensional feature space in which a linear solution is found. 4.2.1 SVR to Predict Travel Time for Urban Road The idea of the regression problem is to determine a function that can approximate (i.e. predict) future values accurately. The generic SVR estimating function takes the form: \\[\\begin{equation} \\operatorname {f(x)} = (w.\\Phi(x)) = b \\tag{4.1} \\end{equation}\\] Where \\(w\\subset R^{n}\\), \\(b\\subset R\\) and \\(\\Phi\\) denotes a nonlinear transformation from \\(R^{n}\\), to high-dimensional space. The kernel function applied to the study is the Gaussian radial basis function (RBF): \\[\\begin{equation} \\displaystyle K(\\mathbf {x_i} ,\\mathbf {y_i} )=\\exp \\left(-{\\frac {\\|\\mathbf {x_i} -\\mathbf {y_i} \\|^{2}}{2\\sigma ^{2}}}\\right) \\tag{4.2} \\end{equation}\\] Where \\(\\sigma &gt; 0\\) is the kernel bandwidth. SVR tries to find a function \\(f(x)\\) where the predicted values are at most \\(\\in\\) from the observed values \\(y_i\\), that is they fit inside a tube of width \\(2\\in\\). Figure 4.3: SVR predictive function(source: Wu and Lee 2004). \\[\\begin{equation} \\text{ minimize } \\frac{1}{2} \\|w\\|^2 \\tag{4.3} \\end{equation}\\] \\[\\begin{equation} \\text{ subject to } \\begin{cases} y_i - \\langle w, x_i \\rangle - b \\le \\varepsilon, \\\\ \\langle w, x_i \\rangle + b - y_i \\le \\varepsilon, \\end{cases} \\tag{4.4} \\end{equation}\\] where \\(x_i\\) is a training sample with target value \\(y_i\\). The inner product plus intercept \\(\\langle w, x_i \\rangle + b\\) is the prediction for that sample, and \\(\\in\\) is a free parameter that serves as a threshold: all predictions have to be within an \\(\\in\\) range of the true predictions. Slack variables are usually added into the above to allow for errors and to allow approximation in the case the above problem is infeasible. To train the SVR models, a better classification could be achieved by optimizing the parameters in some way. There are two parameters to train: the kernel parameter sigma and the constant c. The former is set to automatic by default. The constant C determines the amount of allowable error in the model, controlling the trade-off between model complexity and error. The value of c is not estimated and is set to 1. A large c assigns higher penalties to errors so that the regression is trained to minimize error with lower generalization, while a small assigns fewer penalties to errors. Allowing a large number of errors, the model would be less complex, thus providing higher generalization ability. On the other hand, If c is set with a high number, SVR would not allow the occurrence of any error and results would lead to a complex model. Therefore, it is worthwhile testing some values of c to see how they affect the results. Finally, epsilon determines the width of the tube. 4.3 The Modeling and Solving Approach The first part of the experiment is to train the dataset. The data have been divided into training (80%) and testing (20%) data. Four road links have been aleatory chosen to train the model: IDs 2112, 2061, 473 and 2087. To strategy applied to the model training procedure is composed of four steps presented on Figure 4.4. Figure 4.4: Model Training Procedure steps. To train the data, the package called caret (short for classification and regression training) has been used. The library provides a set of functions that attempt to streamline the process for creating predictive models.12 Step 1: Build a time series SVR Model. The first step is to rearrange the series in order to build a time series SVR model by means of a temporal autoregressive structure. The moving average uses past forecast errors to forecast future values of the series. load(&quot;forecasting.RData&quot;) #first load the data to the space environement library(kernlab) # install if necessary using &#39;install.packages&#39; ## ## Attaching package: &#39;kernlab&#39; ## The following object is masked from &#39;package:ggplot2&#39;: ## ## alpha library(caret) # install if necessary using &#39;install.packages&#39; To build the model, we will use a temporal autoregressive structure. We use embed function to rearrange the series. This means we will forecast the future TT at a single location (link=2112) as a function of a subset of previous TT observations. The resulting matrix can be used for one step ahead forecasting by using the first three columns as the independent variables and the fourth column as the dependent variable. In order to perform parameters tuning with k-fold cross-validation, three different embedding dimensions q have been applied: q=3: q=4 and q=5. We will use the embed to rearrange the series in this way: m &lt;- 3 data.2112 &lt;- embed(x = UJT [,&quot;2112&quot;], dimension = m+1) # embed: function to rearrange the series colnames(data.2112) &lt;- c(&quot;y_t-3&quot;,&quot;y_t-2&quot;,&quot;y_t-1&quot;,&quot;y_t&quot;) The function embeds the time series x into a low-dimensional euclidean space. This way, it is possible to forecast the future temperature as a function of a subset of previous temperature observations. Each row of the resulting matrix consists of sequences \\(x[t], x[t-1], ..., x[t-dimension+1]\\), where \\(t\\) is the original index of \\(x\\). If \\(x\\) is a matrix, (i.e. \\(x\\) contains more than one variable) then \\(x[t]\\) consists of the observation on each variable. The resulting matrix can be inspected with the funcion head () which returns the first or last parts of the matrix. You will see that each row contains four observations, which are four consecutive TT observations. The first row of the matrix contains the first four observations from UJT[,“2112”] in reverse order ( e.g. column 4 is the first observation and column 1 is the 4th observation). In the second row, the first observation is removed from column 4 and the 5th observation is added to column 1. This matrix can be used for one step ahead forecasting by using the first three columns as the independent variables and the fourth column as the dependent variable. # Returns the first or last parts of a vector, matrix, table, data frame or function head (data.2112) ## y_t-3 y_t-2 y_t-1 y_t ## [1,] 0.1212617 0.1152238 0.1101922 0.09610367 ## [2,] 0.1207585 0.1212617 0.1152238 0.11019217 ## [3,] 0.1061669 0.1207585 0.1212617 0.11522378 ## [4,] 0.1111985 0.1061669 0.1207585 0.12126170 ## [5,] 0.1368597 0.1111985 0.1061669 0.12075854 ## [6,] 0.1157269 0.1368597 0.1111985 0.10616689 The same procedure could be applied to the other dimensions q=4 and q=5. m &lt;- 4 data.2112_d2 &lt;- embed(x = UJT [,&quot;2112&quot;], dimension = m+1) colnames(data.2112_d2) &lt;- c(&quot;y_t-4&quot; , &quot;y_t-3&quot;,&quot;y_t-2&quot;,&quot;y_t-1&quot;,&quot;y_t&quot;) m &lt;- 5 data.2112_d3 &lt;- embed(x = UJT [,&quot;2112&quot;], dimension = m+1) colnames(data.2112_d3) &lt;- c(&quot;y_t-5&quot; , &quot;y_t-4&quot; , &quot;y_t-3&quot;,&quot;y_t-2&quot;,&quot;y_t-1&quot;,&quot;y_t&quot;) Step 2: Divide data into training and testing sets. The next step is to divide the embedded data into training and testing sets. The subset of training data is used to fit the model and the testing set to estimate model fitness. It should be noted that in temporal data there is a continuity component whereas it is important to take into account the ordering of time. That said, based on a continuous data series, the first 80% data of the months have to be selected to train the model and the remaining 20% data has been selected to test the model. Hence, the particular training set \\(S= {(X_i,y_i), …,(X_i,y_i)}\\) has been drawn from the distribution of the random vectors \\(y\\) and \\(X\\). # divide data into training and testing sets - Dimension 1 split &lt;- 4143 # UJT row to separe training and testing data yTrain &lt;- data.2112 [1:split,1] XTrain &lt;- data.2112 [1:split,-1] yTest &lt;- data.2112 [(split+1):nrow(data.2112 ),1] XTest &lt;- data.2112 [(split+1):nrow(data.2112 ),-1] # divide data into training and testing sets - Dimension 2 split_d2 &lt;- 4143 #UJT row to separe training and testing data yTrain_d2 &lt;- data.2112_d2 [1:split,1] XTrain_d2 &lt;- data.2112_d2 [1:split,-1] yTest_d2 &lt;- data.2112_d2 [(split+1):nrow(data.2112_d2 ),1] XTest_d2 &lt;- data.2112_d2 [(split+1):nrow(data.2112_d2 ),-1] # divide data into training and testing sets - Dimension 3 split_d3 &lt;- 4143 # UJT row to separe training and testing data yTrain_d3 &lt;- data.2112_d3 [1:split,1] XTrain_d3 &lt;- data.2112_d3 [1:split,-1] yTest_d3 &lt;- data.2112_d3 [(split+1):nrow(data.2112_d3 ),1] XTest_d3 &lt;- data.2112_d3 [(split+1):nrow(data.2112_d3 ),-1] Step 3. Perform parameters tuning with k-fold cross validation. To determine the values of the tuning parameters, one approach is to use a resampling to estimate how well the model performs on the training set. There are different types of resampling methods, being k-fold cross-validation one of the most common types13. The process must be repeated many times and the performance estimates from each holdout set are averaged into a final overall estimate of model efficacy such that given the training set, the algorithm produces a prediction function \\(f(x)= Փ(x_i)\\). For each parameter combination the model fitness is estimated via resampling and the relationship between the tuning parameters and the model performance is evaluated. The k-fold cross-validation has been applied as resampling method, with \\(k\\) set to 5. In k-fold cross validation, the data are randomly partitioned into k-folds. Each fold is left out in turn and the remaining \\(k-1\\) folds are used to train a model and predict its values. The selected model is the one with the best average performance across the k folds14. The procedure prevents overfitting to a subset of the training data. The caret package is used to perform the cross validation with a radial basis function kernel applied for parameter tuning. There are two tuning parameters: the radial basis function scale parameter bandwidth \\(α\\), and the cost value associated with support vectors. To train the model we will use k-fold cross validation, with \\(k\\) set to 5. #To train the model we will use k-fold cross validation, with k set to 5 ctrl &lt;- trainControl(method = &quot;cv&quot;, number=5) To test (at present, caret only allows \\(sigma\\) and \\(C\\) to be trained in this way so we cannot optimise over epsilon automatically) and train the model, a grid of parameters must be created. # Create a grid of parameters to test and train the model with dimension 1 SVRGridCoarse &lt;- expand.grid(.sigma=c(0.001, 0.01, 0.1), .C=c(10,100,1000)) SVRFitCoarse &lt;- train(XTrain, yTrain, method=&quot;svmRadial&quot;, tuneGrid=SVRGridCoarse, trControl=ctrl, type=&quot;eps-svr&quot;) # display results SVRFitCoarse ## Support Vector Machines with Radial Basis Function Kernel ## ## 4143 samples ## 3 predictor ## ## No pre-processing ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 3313, 3315, 3315, 3315, 3314 ## Resampling results across tuning parameters: ## ## sigma C RMSE Rsquared MAE ## 0.001 10 0.01855788 0.7449160 0.01395675 ## 0.001 100 0.01838884 0.7497854 0.01383987 ## 0.001 1000 0.01831466 0.7516992 0.01375272 ## 0.010 10 0.01834316 0.7509672 0.01377398 ## 0.010 100 0.01841338 0.7491764 0.01381130 ## 0.010 1000 0.01844320 0.7484456 0.01383601 ## 0.100 10 0.01854439 0.7456133 0.01388359 ## 0.100 100 0.01887267 0.7366928 0.01400829 ## 0.100 1000 0.01959266 0.7178368 0.01415667 ## ## RMSE was used to select the optimal model using the smallest value. ## The final values used for the model were sigma = 0.001 and C = 1000. Results are given for each of the parameter combinations in terms of the root mean squared error (RMSE), R squared and Mean Absolute Error (MAE). We can view the results visually using the ggplot() function, as displayed in figure 4.5. ggplot(SVRFitCoarse) Figure 4.5: Training errors for the SVR model with different parameter combinations The plot shows that the value of sigma=0.1 and C=1000 provide the best model performance. However, we can refine the grid to see if we can gain further improvements in performance. To test the best performance, we might differentiate the values of the embedding dimension with the three different autoregressive orders have been used: \\(q=3\\); \\(q=4\\) and \\(q=5\\). # Create a grid of parameters to test and train the model with dimension 2 SVRGridCoarse_d2 &lt;- expand.grid(.sigma=c(0.001, 0.01, 0.1), .C=c(10,100,1000)) SVRFitCoarse_d2 &lt;- train(XTrain_d2, yTrain_d2, method=&quot;svmRadial&quot;, tuneGrid=SVRGridCoarse, trControl=ctrl, type=&quot;eps-svr&quot;) SVRFitCoarse_d2 ## Support Vector Machines with Radial Basis Function Kernel ## ## 4143 samples ## 4 predictor ## ## No pre-processing ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 3313, 3314, 3314, 3316, 3315 ## Resampling results across tuning parameters: ## ## sigma C RMSE Rsquared MAE ## 0.001 10 0.01844992 0.7485321 0.01383254 ## 0.001 100 0.01822262 0.7548238 0.01367620 ## 0.001 1000 0.01816424 0.7561625 0.01361397 ## 0.010 10 0.01819337 0.7551913 0.01363658 ## 0.010 100 0.01821017 0.7550758 0.01364336 ## 0.010 1000 0.01819567 0.7555293 0.01365335 ## 0.100 10 0.01838699 0.7497893 0.01371988 ## 0.100 100 0.01867380 0.7415464 0.01388985 ## 0.100 1000 0.02000167 0.7053679 0.01426356 ## ## RMSE was used to select the optimal model using the smallest value. ## The final values used for the model were sigma = 0.001 and C = 1000. # Create a grid of parameters to test and train the modelwith dimension 3 SVRGridCoarse_d3 &lt;- expand.grid(.sigma=c(0.001, 0.01, 0.1), .C=c(10,100,1000)) SVRFitCoarse_d3 &lt;- train(XTrain_d3, yTrain_d3, method=&quot;svmRadial&quot;, tuneGrid=SVRGridCoarse, trControl=ctrl, type=&quot;eps-svr&quot;) SVRFitCoarse_d3 ## Support Vector Machines with Radial Basis Function Kernel ## ## 4143 samples ## 5 predictor ## ## No pre-processing ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 3312, 3315, 3315, 3314, 3316 ## Resampling results across tuning parameters: ## ## sigma C RMSE Rsquared MAE ## 0.001 10 0.01837042 0.7505541 0.01377700 ## 0.001 100 0.01816826 0.7563109 0.01362338 ## 0.001 1000 0.01817603 0.7559690 0.01360780 ## 0.010 10 0.01822833 0.7545035 0.01363603 ## 0.010 100 0.01826660 0.7537413 0.01367489 ## 0.010 1000 0.01833533 0.7522191 0.01373710 ## 0.100 10 0.01856849 0.7463215 0.01388551 ## 0.100 100 0.01933124 0.7263834 0.01423794 ## 0.100 1000 0.02172044 0.6724945 0.01492969 ## ## RMSE was used to select the optimal model using the smallest value. ## The final values used for the model were sigma = 0.001 and C = 100. We now can use ggplot() to plot the results. ggplot(SVRFitCoarse_d2) ggplot(SVRFitCoarse_d3) Figure 4.6: Training errors for the SVR model with different parameter combinations For all dimensions, \\(sigma = 0.001\\) and \\(C = 1000\\) present the best outome, whereas \\(q = 4\\) provides the best fit with \\(RMSE = 0.01813995\\). However, it should be noted that smaller value of sigma could produce a better result. We may achieve a better classification if we try to optimise the parameters in some way. There are two parameters to train: the kernel parameter \\(sigma\\) and the constant \\(C\\). \\(Sigma\\) is set automatic by default. The value of \\(C\\) is not estimated and is arbitrary set to a. Therefore, it is important to test some more values for \\(C\\). We can refine the grid to see if we can gain further improvements in performance. # Testing the grid to see if we can gain further improvements in performance: diferent sigma for d1 SVRGridFine &lt;- expand.grid(.sigma=c(0.005, 0.05, 0.5), .C=c(5,50,500)) system.time(SVRFitFine &lt;- train(XTrain, yTrain, method=&quot;svmRadial&quot;, tuneGrid=SVRGridFine, trControl=ctrl, type=&quot;eps-svr&quot;)) ## user system elapsed ## 144.38 4.30 148.70 # display results SVRFitFine ## Support Vector Machines with Radial Basis Function Kernel ## ## 4143 samples ## 3 predictor ## ## No pre-processing ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 3314, 3314, 3314, 3314, 3316 ## Resampling results across tuning parameters: ## ## sigma C RMSE Rsquared MAE ## 0.005 5 0.01842091 0.7492711 0.01386240 ## 0.005 50 0.01833551 0.7513841 0.01377070 ## 0.005 500 0.01834914 0.7509496 0.01376983 ## 0.050 5 0.01836352 0.7506480 0.01379302 ## 0.050 50 0.01844122 0.7486064 0.01381828 ## 0.050 500 0.01949416 0.7230844 0.01400462 ## 0.500 5 0.01881759 0.7380230 0.01399943 ## 0.500 50 0.01966634 0.7145673 0.01431567 ## 0.500 500 0.02148766 0.6668612 0.01488730 ## ## RMSE was used to select the optimal model using the smallest value. ## The final values used for the model were sigma = 0.005 and C = 50. # plot the results ggplot(SVRFitFine) Figure 4.7: Training errors for the SVR model with different parameter combinations ``` Step 4: Select the model and apply predictive analysis. The effectiveness of a model is commonly measured using a single statistic. For models predicting a numeric outcome, the fitness statistic might be the root mean squared error (RMSE) or the coefficient of determination (R2). For classification, where a discrete outcome is being predicted, the error rate might be an appropriate measure. Examining the results considering the RMSE index, the best performance can be pointed out for the road link. The model object for the best model can be accessed by: SVRFitFine$finalModel ## Support Vector Machine object of class &quot;ksvm&quot; ## ## SV type: eps-svr (regression) ## parameter : epsilon = 0.1 cost C = 50 ## ## Gaussian Radial Basis kernel function. ## Hyperparameter : sigma = 0.005 ## ## Number of Support Vectors : 3359 ## ## Objective Function Value : -58828.6 ## Training error : 0.247802 The model for one-step-ahead prediction can be used to plot the results (figure 4.8). The model forecasts the TTs reasonably well. For road link 2112, the proportion of points used as support vector has been of 62% from the dataset. # We can use the model for one-step-ahead prediction and plot the results yPred &lt;- predict(SVRFitCoarse_d2, XTest_d2) plot(yTest_d2, type=&quot;l&quot;, xaxt=&quot;n&quot;, xlab=&quot;Observations&quot;, ylab=&quot;TTs&quot;) points(yPred, col=&quot;blue&quot;, pch=21, bg=&quot;blue&quot;) #axis(1, at=seq(90, (180*5)+90, 180), labels=unique(dates[(split+1):nrow(data),1])) Figure 4.8: Observed vs predicted TT using an SVR model Finally, it is also important to check whether any temporal autocorrelation remains in the residuals of the model. The autocorelation function (ACF) is used in time series analyses. It is good practice to also check whether any temporal autocorrelation remains in the residuals of the model (figure 4.9) # Check whether any temporal autocorrelation remains in the residuals of the model SVRResidual &lt;- yTest_d2-yPred plot(SVRResidual) Figure 4.9: Residual autocorrelation for SVR model figure 4.10) displays differenced autocorrelation function (ACF) plot for the data of the series. The ACF is usually calculated for a set number of lags. The smaller the lag (separation between observations) the greater the correlation between temperatures. The dashed lines are approximate 95% confident interval for the autocorrelation. It can be notted that no significant overall autocorrelation is presented. Additionally, no seasonal pattern could be noted as well. # To produce an autocorrelation plot acf(SVRResidual) Figure 4.10: Residual autocorrelation for SVR model 4.4 Chapter Summary The basic idea behind the development of SVMs is the same: for a given learning task, with a given finite number of training data, the best generalization performance will be achieved if the right balance is struck between the accuracy attained on that particular training set, and the ability of the algorithm to predict unseen data samples with a minimum of errors. Finally, it is worth mentioning that the experiment applies time series only. It is possible to create a space-time model using the spatial weight matrix but since the spatio-temporal autocorrelation is weaker than the temporal autocorrelation (as previously discussed on the study), the model has not been applied. knitr::opts_chunk$set(error = TRUE) References "],
["artificial-neural-network.html", "5 Artificial Neural Network 5.1 Introduction 5.2 Method Description 5.3 The Modeling and Solving Approach 5.4 Chapter Summary", " 5 Artificial Neural Network 5.1 Introduction Artificial Neural Networks (ANN) are a family of computational systems capable of machine learning and pattern recognition emulating the biological nervous systems (for example the animal brain) (see figure 5.1). The main applications of ANNs involve computer vision, machine translation, social network filtering, speech recognition and medical diagnosis. Figure 5.1: A biological neural network (left) and an artificial neural network (right). (Source: Makinde and Asuquo 2012). ANN has increasingly draws attention within the industry and the academia (for a detailed presentation of ANN, see (Zafeiris and Ball 2018) and also (Wang and Zhong 2016)) 5.2 Method Description Typical ANNs models have a novel structure. In this structure, artificial neurons are aggregated into layers (figure 5.2). The input layer is the leftmost, the hidden layer stands in the middle and the output layer is on the right of the network. The neural network is composed of a number of nodes (the neurons) and weighted connections between the nodes (the synapses). Nodes called artificial neurons in the neural network can range from several to thousands depending on the complexity of the task to achieve. Similarly, the neural network can have a single hidden layer or multiple hidden layers depending on the task. In a neural network, a signal is transmitted from one node to another. The signal received by a node is processed and then signals additional nodes connected to it. This is known as multilayer feed-forward network, where each layer of nodes receives inputs from the previous layer. The input layer contains nodes, variables that encoded the values used to predict an output. The hidden layer contains nodes that do the calculation on the signal passed from the input node and pass the result to the output node(s). In a multiple hidden layer network, the signal is passed to more than one hidden layer before being passed to the output node. The output node is the variable(s) that we intend to predict. Figure 5.2: Sketch of a feed-forward artificial neural network. The hidden neuron is responsible for the calculation of the weighted of all the inputs (a weighted linear combination), adds a bias and then decides whether to fire a neuron sending the signal along the axon (figures 5.2 and 5.3). At each connection, called synapses or edge, between nodes, the transmitted signal will correspond to a real number (Gerven and Bohte 2018). This number is denominated weight and the learning process can be carried-on. The increase or decrease of this weight influences the strength of the signal at the connection. The inputs to each node are aggregated using a weight linear combination (an analytical method applied when more than one attribute is taken into account). If the summation of equation (5.1) at the node is greater than a certain threshold value, the node transmits a signal along the axon to the next nodes connected to it. The output of each artificial neuron is modified by a non-linear function of the weighted sum of its inputs plus a bias (figure 5.3). Figure 5.3: Sketch map of operations inside the hidden neuron. \\[\\begin{equation} {\\displaystyle \\operatorname {z_n} =\\sum _{n=1}w_{kn} x_n+ b_k} \\tag{5.1} \\end{equation}\\] In equation (5.1), the parameters \\(b_1, b_2, b_3\\) and \\(w_{1,1}, ..., w_{4,3}\\) are “learned” from the data. The values of the weights are often restricted to prevent them from becoming too large. The parameter that restricts the weights is known as the decay parameter and is often set to be equal to 0.1. In the hidden layer, these non-linear functions or activation functions, like the sigmoid function in (5.2), are used to complete the transformation from input nodes to an output layer. This transformation is carried out to reduce the effects of extreme input values making the network more robust and less susceptible to outliers. \\[\\begin{equation} \\text s(z) ={\\frac {1}{1+e^-z}} \\tag{5.2} \\end{equation}\\] The weights take random values to begin with, and these are then updated using the observed data. The network is usually trained several times using different random starting points, and the results are averaged. The number of hidden layers, and the number of nodes in each hidden layer, must be specified in advance. ANN uses several models such as feed-forward and feedback propagation for prediction. Information moves on single direction for feed-forward network with no cycles or loops in the network. Supervised learning is employed in feedback propagation, requiring training data for predictive analysis. Figure 5.4 illustrates a standard process for ANN. The input data is used to training and testing procedures. After applying the model, the accuracy assessment must be done to evaluate the model. Figure 5.4: ANN modelling framework (Source: Qazi and Khan 2015). 5.3 The Modeling and Solving Approach In this forecasting experiment, we will use supervised learning to train the dataset. As already mentioned in this book, supervised learning is applied by giving a set of labelled training data to the algorithm to predict labels for unseen data. This is achieved by first dividing the dataset into training and test using 80%:20% ratio. The first step we need to take prior to any command is to load the data to the space environement and to install some required packages. There are several ANN libraries in R. The commonly used ones are amore, rsnns, neuralnet and nnet, the most widely used general purpose library for neural network analysis15 # install if necessary using &#39;install.packages&#39; # we will use a function from github library(nnet) source_url(&#39;https://gist.githubusercontent.com/fawda123/7471137/raw/466c1474d0a505ff044412703516c34f1a4684a5/nnet_plot_update.r&#39;) # also, load the data to the space environement load(&quot;forecasting.RData&quot;) tts &lt;- read_excel(&quot;tts.xlsx&quot;, sheet = &quot;transpose&quot;, col_names = FALSE) In this chapter, the same procedure of the former exercises will be applied: 23 days as training data and 7 days as testing. Also, we will apply the feed-forward model, wherein connections between the nodes do not form a cycle, a different approach to recurrent neural networks. In this model, ANNs allow signals to travel one way only: from input to output. There are no feedback (loops), that means that the output of any layer does not affect that same layer. Feed-forward ANNs tend to be straightforward networks that associate inputs with outputs. They are extensively used in pattern recognition.16 In the experiment, the training data has to be split into two separate datasets so that we can use the previous 5-minute interval values to predict the next interval. A multiple-input and multiple-output ANN structure have to be deployed using the previous 5-minute interval for the unit travel time (UTT) values for all road links. This process is iterated so as to obtain 4140 observations which is the equivalent to 23 days split at a 5-minute interval. UJT.chosen &lt;- UJT[ ,c(&quot;1745&quot;,&quot;1882&quot;,&quot;1622&quot;,&quot;1412&quot;,&quot;1413&quot;,&quot;1419&quot;,&quot;2090&quot;,&quot;2054&quot;,&quot;2112&quot;, &quot;2357&quot;,&quot;2364&quot;,&quot;2059&quot;,&quot;2061&quot;,&quot;2062&quot;)] # Divide dataset into training and testing data training &lt;- UJT.chosen[1:4140,] testing &lt;- UJT.chosen[4141:5400,] training_1 &lt;-training[-1,] The following step after training the data involves setting the best tune parameters to carry out the prediction. The parameters that mut be set are: 1. decay (i.e. the weight decay) and 2. size (i.e. the number of neurons in the hidden layer). In this exercise, we will use a single hidden layer. To ensure we get a good model, it is wise to test a range of values of each of the model parameters. To do this we will use the caret package. mygrid is the selection of parameters that will be tested. To optimize the parameters, a grid (i.e. expanded grid) with a set of decay and size values ranging from 5e-06 to 5e-01 and 1 to 10, are defined. Moreover, we are going to set a 10-fold cross-validation repeated 3 times. All this process must be carries-out to find the optimal model parameter (i.e. the decay and size) to be applied. set.seed(6) mygrid &lt;- expand.grid(.decay=c(0.000005,0.00005,0.0005,0.005,0.05,0.5), .size=c(1:10)) fitControl &lt;- trainControl(&quot;repeatedcv&quot;, number = 10, repeats = 3, returnResamp = &quot;all&quot;) set.seed(6) nnet.parameters &lt;- train(training[1:4139,1:14], training_1[,14], method = &quot;nnet&quot;, metric = &quot;log&quot;, trControl = fitControl, tuneGrid=mygrid) plot(nnet.parameters) Figure 5.5: RMSE error in the traing data simulation using different size and decay parameters. Figure 5.5 displays the results of this process showing the change in error (RMSE) for the different weight decay and size (#Hidden Units). The same figure also reveals that weight 5e-03 and size 7 are the best-tunned parameters for this model. The final step involves the prediction and validation of the model using unseen testing dataset. The prediction can be obtained using the best-tuned parameter as mentioned. Figure 5.6 presents the internal structure of the neural network using the best-tuned parameters. Once the prediction for all the road links has been carried on, we must validate the model by assessing the observed results with the predicted ones. This is done comparing the mean absolute percent error (MAPE) and the mean square error (MSE) index generated between the predict values and observed values (Table XXXX). plot.nnet(nnet.parameters$finalModel) Figure 5.6: Internal structure of the ANN using the best-tuned parameters. # Predict using best tune parameters set.seed(6) temp.nnet &lt;- nnet(training[1:3960,], training_1[1:3960,], decay=5e-3, linout = TRUE, size=7) set.seed(6) temp.pred&lt;-predict(temp.nnet, testing) Here the decay parameter refers to the weight decay, the linout parameter switches to linear output units, and as our task is to do regression this parameter should be TRUE (the alternative is logistic units for classification). The size parameter refers to the number of neurons in the hidden layer. To look at the parameters you can control in nnet, consult the documentation using ?nnet on the console panel. We finally can analyse the exercise results. Figure 5.7 displays the fit result for road link 2087 and error statistics for all can be accessed on table XXXX. We can note that the RMSE error for road links 473 and 2061 higher were higher (i.e. 0.07 and 0.11 in turns) compared to RMSE errors in road links 2087 and 2112, 0.02 and 0.04 respectively. This is also consistent with the MAPE errors values generated by the four-road links (Table.3.4.1). MAPE errors in road links 473 and 2061 were also higher compared to road link 2087 and 2112. This means that road links 2087 and 2112 are more forecastable compared to road links 473 and 2061. matplot(cbind(testing[,1], temp.pred[,1]),ylab=&quot;UTT&quot;, xlab=&quot;Lags&quot;, main=&quot;Road link 2087&quot;, type=&quot;l&quot;, lty = c(1,1), col = c(&quot;blue&quot;,&quot;red&quot;)) legend(1,.35, legend = c(&quot;Test&quot;,&quot;Prediction&quot;), lty = c(1,1), col = c(&quot;blue&quot;,&quot;red&quot;),box.lwd = .5, cex = .65 ) Figure 5.7: Observed versus forecast values for road link 2087 for 5-minute interval. This chapter introduced ANNs for time series modelling. Artificial neural networks are forecasting methods that are based on simple mathematical models of the brain. ANNs has shown to be very appropriated tool in many applications that involve dealing with complex real world sensor data. A common criticism of neural networks, particularly in robotics, is that they require too much training for real-world operation. Nonetheless, because of their ability to reproduce and model nonlinear processes, Artificial neural networks have found many applications in a wide range of disciplines such as system identification and control (vehicle control or trajectory prediction), quantum chemistry,[207] general game playing, face identification and data mining. 5.4 Chapter Summary This chapter introduced ANNs for time series modelling. Artificial neural networks are forecasting methods that are based on simple mathematical models of the brain. ANNs has shown to be very appropriated tool in many applications that involve dealing with complex real world sensor data. A common criticism of neural networks, particularly in robotics, is that they require too much training for real-world operation. Nonetheless, because of their ability to reproduce and model nonlinear processes, artificial neural networks have found many applications in a wide range of disciplines such as system identification and control (vehicle control or trajectory prediction), quantum chemistry, general game playing,face identification and data mining. References "],
["summing-up.html", "6 Summing-up", " 6 Summing-up Network science literature has gained popularity to a broad spectrum of areas such as engineering, economics and geography over the last decade. There is a wide range of quantitative forecasting methods, often developed within specific disciplines for specific purposes. Each method has its own properties, accuracies, and costs that must be considered when choosing a specific method. We can highlight some recommendations for each model when dealing with a forecasting problem. Certain aspects of each model could be improved and therefore the strengths and weaknesses of each model must be pointed out. To improve accuracy results of STLF method, it is essential to perform a fine-tuning process. Each parameter of the model must be adjusted very precisely in order to gain a better accuracy accross different values of lambda. They should be tested for the BoxCox transformations for each time series variable individually. Certain aspects of the whole process are highly computationally expensive; the method could only be ran for a very small number of lags due to the large amount of calculations nested within the function. Even though for this case STLF the approach was STL+ETS, there are more methods that could be assessed like for example STL+THETAF or any other function defined by the user that makes use of time series. Finally, the uncertainty of the model could be reduced, and its performance could increase if forecasting for fewer steps ahead or by aggrerating the data in larger lags but with the risk of losing the granularity of the data. Support vector regression offers a good generalization performance largely due to the ability to control two different factors: the error-rate on the training data and the capacity of the learning machine to predict unseen data samples. The generalization ability is indeed a key attraction to SVR, largely due to the structural risk minimization (SRM) principle: a strategy to tackle the trade-off between the confidence interval of the VC-dimension and the value of the error frequency. Another merit to SVR is the model tuning process that can be streamlined by adaptively resampling methods as k-fold cross-validation. This way, settings that are clearly sub-optimal can be discarded, leading to support vectors that benefits of robustness and the generalization ability to integrate new information into the existing structure of the solution. On the other hand, some comments can be drawn about the limitations and improvements. Training for large datasets represents a serious limitation for performance, in particular for speed and parameters control. The process of tuning parameters is time-consuming even though benefiting of the versatility of the caret package for R. For artificial neural network, a good way to approach and improve the outcomes of the methods is by changing the set of parameters such as size, weight decay and a number of hidden layers. However, in this case, we can discard changing the weight decay and the size assuming that the best-tuned parameter for both selected during the trading phase. Also, the increasing of the number of hidden layers must be considered in order to improve the ability to identify more complex patterns. In addition to this, aggregating the data at longer interval of time is also a good strategy to avoid cumulative prediction error. However, one thing to note in these non-parametric methods is that selecting the appropriate number of nodes or interpret their structure can be counter-intuitive which are often referred to as black boxes. It is important to test the feasibility and quality of each model. The testing process can give important new insights into the nature of the forecasting problem. The testing process is also important because it provides the opportunity to doublecheck the robustness and validity of the model. Also, testing the results against known results helps ensure the structural integrity and validity of the model. After all, we can always come to the conclusion that we need to go back and modify the model formulation and implementation. "],
["appendix.html", "7 Appendix A Setting-up R B Data set", " 7 Appendix A Setting-up R R can be downloaded and installed here:https://www.rstudio.com/ or from any CRAN (the Comprehensive R Archive Network) mirrors at https://cran.rstudio.com/. Code RStudio is entered into the console or into the script editor. We recommend to enter the code in the console, but once you have written code that works and does what you want, put it in the script editor. RStudio will automatically save the contents of the editor when you quit RStudio, and will automatically load it when you re-open. RStudio comes with a number of pre-installed packages They are stored on a repository called CRAN (the Comprehensive R Archive Network): https://cran.r-project.org/. Packages contributed to CRAN have gone through certain checks to ensure they are stable. Libraries from CRAN need to be installed first using install.packages('package_name') before they can be loaded using library(). To have its functions accessible, a library must be loaded even if already installed. R packages are also often constantly updated on CRAN or GitHub, so you may want to update them once in a while: update.packages(ask = FALSE) Although it is not required, the RStudio IDE can make a lot of things much easier when you work on R-related projects. The RStudio IDE can be downloaded from link. The data used in the exercises are on Github and can be downloaded at http://laurentlsantos.github.io/forecasting/ To ensure the file paths work correctly, first change your working directory to the location where you saved the data using setwd(). This book was built with: ## - Session info ---------------------------------------------------------- ## setting value ## version R version 3.6.0 (2019-04-26) ## os Windows 10 x64 ## system x86_64, mingw32 ## ui RTerm ## language (EN) ## collate English_United Kingdom.1252 ## ctype English_United Kingdom.1252 ## tz Europe/London ## date 2019-07-12 ## ## - Packages -------------------------------------------------------------- ## package * version date lib source ## askpass 1.1 2019-01-13 [1] CRAN (R 3.6.0) ## assertthat 0.2.1 2019-03-21 [1] CRAN (R 3.6.0) ## backports 1.1.4 2019-04-10 [1] CRAN (R 3.6.0) ## base64enc 0.1-3 2015-07-28 [1] CRAN (R 3.6.0) ## BH 1.69.0-1 2019-01-07 [1] CRAN (R 3.6.0) ## broom 0.5.2 2019-04-07 [1] CRAN (R 3.6.0) ## callr 3.2.0 2019-03-15 [1] CRAN (R 3.6.0) ## cellranger 1.1.0 2016-07-27 [1] CRAN (R 3.6.0) ## cli 1.1.0 2019-03-19 [1] CRAN (R 3.6.0) ## clipr 0.6.0 2019-04-15 [1] CRAN (R 3.6.0) ## colorspace 1.4-1 2019-03-18 [1] CRAN (R 3.6.0) ## crayon 1.3.4 2017-09-16 [1] CRAN (R 3.6.0) ## curl 3.3 2019-01-10 [1] CRAN (R 3.6.0) ## DBI 1.0.0 2018-05-02 [1] CRAN (R 3.6.0) ## dbplyr 1.4.1 2019-06-05 [1] CRAN (R 3.6.0) ## digest 0.6.19 2019-05-20 [1] CRAN (R 3.6.0) ## dplyr * 0.8.3 2019-07-04 [1] CRAN (R 3.6.1) ## ellipsis 0.1.0 2019-02-19 [1] CRAN (R 3.6.0) ## evaluate 0.14 2019-05-28 [1] CRAN (R 3.6.0) ## fansi 0.4.0 2018-10-05 [1] CRAN (R 3.6.0) ## forcats 0.4.0 2019-02-17 [1] CRAN (R 3.6.0) ## fs 1.3.1 2019-05-06 [1] CRAN (R 3.6.0) ## generics 0.0.2 2018-11-29 [1] CRAN (R 3.6.0) ## ggplot2 * 3.1.1 2019-04-07 [1] CRAN (R 3.6.0) ## glue 1.3.1 2019-03-12 [1] CRAN (R 3.6.0) ## gtable 0.3.0 2019-03-25 [1] CRAN (R 3.6.0) ## haven 2.1.0 2019-02-19 [1] CRAN (R 3.6.0) ## highr 0.8 2019-03-20 [1] CRAN (R 3.6.0) ## hms 0.4.2 2018-03-10 [1] CRAN (R 3.6.0) ## htmltools 0.3.6 2017-04-28 [1] CRAN (R 3.6.0) ## httr 1.4.0 2018-12-11 [1] CRAN (R 3.6.0) ## jsonlite 1.6 2018-12-07 [1] CRAN (R 3.6.0) ## knitr * 1.23 2019-05-18 [1] CRAN (R 3.6.0) ## labeling 0.3 2014-08-23 [1] CRAN (R 3.6.0) ## lattice * 0.20-38 2018-11-04 [1] CRAN (R 3.6.0) ## lazyeval 0.2.2 2019-03-15 [1] CRAN (R 3.6.0) ## lubridate 1.7.4 2018-04-11 [1] CRAN (R 3.6.0) ## magrittr 1.5 2014-11-22 [1] CRAN (R 3.6.0) ## markdown 1.0 2019-06-07 [1] CRAN (R 3.6.0) ## MASS * 7.3-51.4 2019-03-31 [2] CRAN (R 3.6.0) ## Matrix 1.2-17 2019-03-22 [1] CRAN (R 3.6.1) ## mgcv 1.8-28 2019-03-21 [2] CRAN (R 3.6.0) ## mime 0.6 2018-10-05 [1] CRAN (R 3.6.0) ## modelr 0.1.4 2019-02-18 [1] CRAN (R 3.6.0) ## munsell 0.5.0 2018-06-12 [1] CRAN (R 3.6.0) ## nlme * 3.1-139 2019-04-09 [2] CRAN (R 3.6.0) ## openssl 1.4 2019-05-31 [1] CRAN (R 3.6.0) ## pillar 1.4.2 2019-06-29 [1] CRAN (R 3.6.1) ## pkgconfig 2.0.2 2018-08-16 [1] CRAN (R 3.6.0) ## plogr 0.2.0 2018-03-25 [1] CRAN (R 3.6.0) ## plyr 1.8.4 2016-06-08 [1] CRAN (R 3.6.0) ## prettyunits 1.0.2 2015-07-13 [1] CRAN (R 3.6.0) ## processx 3.3.1 2019-05-08 [1] CRAN (R 3.6.0) ## progress 1.2.2 2019-05-16 [1] CRAN (R 3.6.0) ## ps 1.3.0 2018-12-21 [1] CRAN (R 3.6.0) ## purrr 0.3.2 2019-03-15 [1] CRAN (R 3.6.0) ## R6 2.4.0 2019-02-14 [1] CRAN (R 3.6.0) ## RColorBrewer 1.1-2 2014-12-07 [1] CRAN (R 3.6.0) ## Rcpp 1.0.1 2019-03-17 [1] CRAN (R 3.6.0) ## readr 1.3.1 2018-12-21 [1] CRAN (R 3.6.0) ## readxl * 1.3.1 2019-03-13 [1] CRAN (R 3.6.0) ## rematch 1.0.1 2016-04-21 [1] CRAN (R 3.6.0) ## reprex 0.3.0 2019-05-16 [1] CRAN (R 3.6.0) ## reshape2 1.4.3 2017-12-11 [1] CRAN (R 3.6.0) ## rlang 0.4.0 2019-06-25 [1] CRAN (R 3.6.0) ## rmarkdown 1.13 2019-05-22 [1] CRAN (R 3.6.0) ## rstudioapi 0.10 2019-03-19 [1] CRAN (R 3.6.0) ## rvest 0.3.4 2019-05-15 [1] CRAN (R 3.6.0) ## scales * 1.0.0 2018-08-09 [1] CRAN (R 3.6.0) ## selectr 0.4-1 2018-04-06 [1] CRAN (R 3.6.0) ## stringi 1.4.3 2019-03-12 [1] CRAN (R 3.6.0) ## stringr 1.4.0 2019-02-10 [1] CRAN (R 3.6.0) ## sys 3.2 2019-04-23 [1] CRAN (R 3.6.0) ## tibble 2.1.3 2019-06-06 [1] CRAN (R 3.6.0) ## tidyr 0.8.3 2019-03-01 [1] CRAN (R 3.6.0) ## tidyselect 0.2.5 2018-10-11 [1] CRAN (R 3.6.0) ## tidyverse 1.2.1 2017-11-14 [1] CRAN (R 3.6.0) ## tinytex 0.13 2019-05-14 [1] CRAN (R 3.6.0) ## utf8 1.1.4 2018-05-24 [1] CRAN (R 3.6.0) ## vctrs 0.1.0 2018-11-29 [1] CRAN (R 3.6.0) ## viridisLite 0.3.0 2018-02-01 [1] CRAN (R 3.6.0) ## whisker 0.3-2 2013-04-28 [1] CRAN (R 3.6.0) ## withr 2.1.2 2018-03-15 [1] CRAN (R 3.6.0) ## xfun 0.7 2019-05-14 [1] CRAN (R 3.6.0) ## xml2 1.2.0 2018-01-24 [1] CRAN (R 3.6.0) ## yaml 2.2.0 2018-07-25 [1] CRAN (R 3.6.0) ## zeallot 0.1.0 2018-01-28 [1] CRAN (R 3.6.0) ## ## [1] C:/Users/Laurent/Documents/R/win-library/3.6 ## [2] C:/Program Files/R/R-3.6.0/library B Data set dates: The dates of data collection. The first column is the date and the second column is the time of day, from 1 (6am) to 180 (9pm). This is the order in which the data appear in UJT. LCAPAdj: The adjacency matrix, where 1 indicates two links are adjacent. Note that the network is not fully connected as some links have been removed due to poor data quality. LCAPShp: A shapefile of the data, which can be quickly visualised usin gplot(LCAPSh) . This shapefile contains all links (1402) so you may want to extract/identify those links for which you have data. The IDs correspond to the LCAP_ID field in LCAPShp@data. tts.xls: The adjusted weighted adjacency matrix. UJT: The data matrix, with one road link per column. The column names are the road link IDs. "],
["references.html", "8 References", " 8 References "]
]
