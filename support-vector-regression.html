<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>4 Support Vector Regression | Introduction to Spatial Network Forecast with R</title>
  <meta name="description" content="4 Support Vector Regression | Introduction to Spatial Network Forecast with R" />
  <meta name="generator" content="bookdown 0.13 and GitBook 2.6.7" />

  <meta property="og:title" content="4 Support Vector Regression | Introduction to Spatial Network Forecast with R" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="4 Support Vector Regression | Introduction to Spatial Network Forecast with R" />
  
  
  

<meta name="author" content="Laurent L. Santos and Francisco S. Castillo" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="seasonal-and-trend-decomposition-with-loess-forecasting-model-stlf.html"/>
<link rel="next" href="artificial-neural-network.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/htmlwidgets-1.3/htmlwidgets.js"></script>
<link href="libs/leaflet-1.3.1/leaflet.css" rel="stylesheet" />
<script src="libs/leaflet-1.3.1/leaflet.js"></script>
<link href="libs/leafletfix-1.0.0/leafletfix.css" rel="stylesheet" />
<script src="libs/Proj4Leaflet-1.0.1/proj4-compressed.js"></script>
<script src="libs/Proj4Leaflet-1.0.1/proj4leaflet.js"></script>
<link href="libs/rstudio_leaflet-1.3.1/rstudio_leaflet.css" rel="stylesheet" />
<script src="libs/leaflet-binding-2.0.2/leaflet.js"></script>
<script src="libs/leaflet-providers-1.1.17/leaflet-providers.js"></script>
<script src="libs/leaflet-providers-plugin-2.0.2/leaflet-providers-plugin.js"></script>
<script src="libs/plotly-binding-4.9.0/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.0.0/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.0.0/js/crosstalk.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-1.46.1/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-1.46.1/plotly-latest.min.js"></script>


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#motivations-and-approach"><i class="fa fa-check"></i>Motivations and Approach</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#prerequisites"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#conventions"><i class="fa fa-check"></i>Conventions</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#online-version"><i class="fa fa-check"></i>Online Version</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#about-the-authors"><i class="fa fa-check"></i>About the Authors</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="getting-started.html"><a href="getting-started.html"><i class="fa fa-check"></i><b>1</b> Getting Started</a><ul>
<li class="chapter" data-level="1.1" data-path="getting-started.html"><a href="getting-started.html#introduction"><i class="fa fa-check"></i><b>1.1</b> Introduction</a></li>
<li class="chapter" data-level="1.2" data-path="getting-started.html"><a href="getting-started.html#network"><i class="fa fa-check"></i><b>1.2</b> Network</a><ul>
<li class="chapter" data-level="1.2.1" data-path="getting-started.html"><a href="getting-started.html#network-fundamentals"><i class="fa fa-check"></i><b>1.2.1</b> Network: fundamentals</a></li>
<li class="chapter" data-level="1.2.2" data-path="getting-started.html"><a href="getting-started.html#road-network-and-travel-time-as-a-variable-for-transportation"><i class="fa fa-check"></i><b>1.2.2</b> Road Network and Travel Time as a Variable for Transportation</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="getting-started.html"><a href="getting-started.html#forecasting"><i class="fa fa-check"></i><b>1.3</b> Forecasting</a><ul>
<li class="chapter" data-level="1.3.1" data-path="getting-started.html"><a href="getting-started.html#space-time-series-forecasting"><i class="fa fa-check"></i><b>1.3.1</b> Space-time Series Forecasting</a></li>
<li class="chapter" data-level="1.3.2" data-path="getting-started.html"><a href="getting-started.html#forecasting-models-of-spatial-network-data"><i class="fa fa-check"></i><b>1.3.2</b> Forecasting Models of Spatial Network Data</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="getting-started.html"><a href="getting-started.html#forecasting-data-and-methods"><i class="fa fa-check"></i><b>1.4</b> Forecasting Data and Methods</a><ul>
<li class="chapter" data-level="1.4.1" data-path="getting-started.html"><a href="getting-started.html#data-description"><i class="fa fa-check"></i><b>1.4.1</b> Data Description</a></li>
<li class="chapter" data-level="1.4.2" data-path="getting-started.html"><a href="getting-started.html#applied-methodologies"><i class="fa fa-check"></i><b>1.4.2</b> Applied Methodologies</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="getting-started.html"><a href="getting-started.html#chapter-summary"><i class="fa fa-check"></i><b>1.5</b> Chapter Summary</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="exploratory-spatio-temporal-data-analysis-and-visualisation.html"><a href="exploratory-spatio-temporal-data-analysis-and-visualisation.html"><i class="fa fa-check"></i><b>2</b> Exploratory Spatio-temporal Data Analysis and Visualisation</a><ul>
<li class="chapter" data-level="2.1" data-path="exploratory-spatio-temporal-data-analysis-and-visualisation.html"><a href="exploratory-spatio-temporal-data-analysis-and-visualisation.html#introduction-1"><i class="fa fa-check"></i><b>2.1</b> Introduction</a></li>
<li class="chapter" data-level="2.2" data-path="exploratory-spatio-temporal-data-analysis-and-visualisation.html"><a href="exploratory-spatio-temporal-data-analysis-and-visualisation.html#examining-data-patterns-and-characteristics"><i class="fa fa-check"></i><b>2.2</b> Examining Data Patterns and Characteristics</a><ul>
<li class="chapter" data-level="2.2.1" data-path="exploratory-spatio-temporal-data-analysis-and-visualisation.html"><a href="exploratory-spatio-temporal-data-analysis-and-visualisation.html#non-spatio-temporal-data-patterns"><i class="fa fa-check"></i><b>2.2.1</b> Non Spatio-temporal Data Patterns</a></li>
<li class="chapter" data-level="2.2.2" data-path="exploratory-spatio-temporal-data-analysis-and-visualisation.html"><a href="exploratory-spatio-temporal-data-analysis-and-visualisation.html#temporal-patterns"><i class="fa fa-check"></i><b>2.2.2</b> Temporal Patterns</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="exploratory-spatio-temporal-data-analysis-and-visualisation.html"><a href="exploratory-spatio-temporal-data-analysis-and-visualisation.html#spatio-temporal-dependence-and-autocorrelation"><i class="fa fa-check"></i><b>2.3</b> Spatio-temporal Dependence and Autocorrelation</a><ul>
<li class="chapter" data-level="2.3.1" data-path="exploratory-spatio-temporal-data-analysis-and-visualisation.html"><a href="exploratory-spatio-temporal-data-analysis-and-visualisation.html#temporal-autocorrelation"><i class="fa fa-check"></i><b>2.3.1</b> Temporal autocorrelation</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="exploratory-spatio-temporal-data-analysis-and-visualisation.html"><a href="exploratory-spatio-temporal-data-analysis-and-visualisation.html#chapter-summary-1"><i class="fa fa-check"></i><b>2.4</b> Chapter Summary</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="seasonal-and-trend-decomposition-with-loess-forecasting-model-stlf.html"><a href="seasonal-and-trend-decomposition-with-loess-forecasting-model-stlf.html"><i class="fa fa-check"></i><b>3</b> Seasonal and Trend Decomposition with Loess Forecasting Model (STLF)</a><ul>
<li class="chapter" data-level="3.1" data-path="seasonal-and-trend-decomposition-with-loess-forecasting-model-stlf.html"><a href="seasonal-and-trend-decomposition-with-loess-forecasting-model-stlf.html#introduction-2"><i class="fa fa-check"></i><b>3.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2" data-path="seasonal-and-trend-decomposition-with-loess-forecasting-model-stlf.html"><a href="seasonal-and-trend-decomposition-with-loess-forecasting-model-stlf.html#method-description"><i class="fa fa-check"></i><b>3.2</b> Method Description</a></li>
<li class="chapter" data-level="3.3" data-path="seasonal-and-trend-decomposition-with-loess-forecasting-model-stlf.html"><a href="seasonal-and-trend-decomposition-with-loess-forecasting-model-stlf.html#the-modeling-and-solving-approach"><i class="fa fa-check"></i><b>3.3</b> The Modeling and Solving Approach</a></li>
<li class="chapter" data-level="3.4" data-path="seasonal-and-trend-decomposition-with-loess-forecasting-model-stlf.html"><a href="seasonal-and-trend-decomposition-with-loess-forecasting-model-stlf.html#chapter-summary-2"><i class="fa fa-check"></i><b>3.4</b> Chapter Summary</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="support-vector-regression.html"><a href="support-vector-regression.html"><i class="fa fa-check"></i><b>4</b> Support Vector Regression</a><ul>
<li class="chapter" data-level="4.1" data-path="support-vector-regression.html"><a href="support-vector-regression.html#introduction-3"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="support-vector-regression.html"><a href="support-vector-regression.html#method-description-1"><i class="fa fa-check"></i><b>4.2</b> Method Description</a><ul>
<li class="chapter" data-level="4.2.1" data-path="support-vector-regression.html"><a href="support-vector-regression.html#svr-to-predict-travel-time-for-urban-road"><i class="fa fa-check"></i><b>4.2.1</b> SVR to Predict Travel Time for Urban Road</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="support-vector-regression.html"><a href="support-vector-regression.html#the-modeling-and-solving-approach-1"><i class="fa fa-check"></i><b>4.3</b> The Modeling and Solving Approach</a></li>
<li class="chapter" data-level="4.4" data-path="support-vector-regression.html"><a href="support-vector-regression.html#chapter-summary-3"><i class="fa fa-check"></i><b>4.4</b> Chapter Summary</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="artificial-neural-network.html"><a href="artificial-neural-network.html"><i class="fa fa-check"></i><b>5</b> Artificial Neural Network</a><ul>
<li class="chapter" data-level="5.1" data-path="artificial-neural-network.html"><a href="artificial-neural-network.html#introduction-4"><i class="fa fa-check"></i><b>5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2" data-path="artificial-neural-network.html"><a href="artificial-neural-network.html#method-description-2"><i class="fa fa-check"></i><b>5.2</b> Method Description</a></li>
<li class="chapter" data-level="5.3" data-path="artificial-neural-network.html"><a href="artificial-neural-network.html#the-modeling-and-solving-approach-2"><i class="fa fa-check"></i><b>5.3</b> The Modeling and Solving Approach</a></li>
<li class="chapter" data-level="5.4" data-path="artificial-neural-network.html"><a href="artificial-neural-network.html#chapter-summary-4"><i class="fa fa-check"></i><b>5.4</b> Chapter Summary</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="summary.html"><a href="summary.html"><i class="fa fa-check"></i><b>6</b> Summary</a></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i>Appendix</a><ul>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#a-setting-up-r"><i class="fa fa-check"></i>A Setting-up R</a></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#b-data-set"><i class="fa fa-check"></i>B Data set</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Spatial Network Forecast with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="support-vector-regression" class="section level1">
<h1><span class="header-section-number">4</span> Support Vector Regression</h1>
<hr />
<div id="introduction-3" class="section level2">
<h2><span class="header-section-number">4.1</span> Introduction</h2>
<p>In this section, the methodology <em>Support Vector Regression</em> (SVR) is applied for travel-time prediction. SVR is a computational technique that has its root on <em>machine learning</em> (ML) methodologies.</p>
<p>The feasibility of applying SVM in travel-time prediction is demonstrated in this section, the results and analysis are further presented.</p>
</div>
<div id="method-description-1" class="section level2">
<h2><span class="header-section-number">4.2</span> Method Description</h2>
<p>SVR is an extension of ML technique known as <em>support vector machine</em> (SVM) to regression problems. SVM makes use of a hypothesis space of linear functions in a feature space, trained with a learning algorithm from optimisation theory. An important aspect of SVM is that not all the available training examples are used in the training algorithm. The subset of points that lies on the margin, called <em>support vectors</em> are the only ones that defines the hyperplane of maximin margin.</p>
<p>SVMs techniques are grounded in the basic principle that for a given learning task, with a given finite number of training data, the best generalization performance will be achieved if the right balance is struck between the accuracy attained and the capacity of the machine to learn any training set without error <span class="citation">(Burges <a href="#ref-R-burges">1998</a>)</span>.<a href="#fn11" class="footnoteRef" id="fnref11"><sup>11</sup></a> Figure <a href="support-vector-regression.html#fig:chunk-svr">4.1</a> presents the classic example of linear support vector machine.</p>
<div class="figure" style="text-align: center"><span id="fig:chunk-svr"></span>
<img src="Images/SVR.png" alt="Linear support vector machine *[source: @R-burges]*" width="70%" />
<p class="caption">
Figure 4.1: Linear support vector machine <em><span class="citation">(source: Burges <a href="#ref-R-burges">1998</a>)</span></em>
</p>
</div>
<p>SVM algorithms use a set of kernel functions. The function of kernel is to take data as input and transform it into the required form. Kernel methods in ML are used in a different way comparing to spatial analysis (Kernel Density Estimation, KDE). Instead of being applied to spatial distances, they are applied to distances between sets of variables. Different SVM algorithms use different types of kernel functions, for example linear, nonlinear, polynomial, radial basis function (RBF), and sigmoid. Figure <a href="support-vector-regression.html#fig:chunk-kernel">4.2</a> presents the basic scheme of kernel function method applied to ML.</p>
<div class="figure" style="text-align: center"><span id="fig:chunk-kernel"></span>
<img src="Images/kernel.png" alt="Kernel to machine learning *[source: @R-cheng2]*" width="85%" />
<p class="caption">
Figure 4.2: Kernel to machine learning <em><span class="citation">(source: Cheng and Haworth <a href="#ref-R-cheng2">2019</a>)</span></em>
</p>
</div>
<p>A kernel function is usually used to refer to the <em>kernel trick</em>, a method of using a linear classifier to solve a non-linear problem. It entails transforming linearly inseparable data to linearly separable ones. Machine learning methods are widely applied to classification and regression problems. The best-known kernel method for regression is support vector regression (SVR), which is based on the principles of <em>statistical learning theory</em> <span class="citation">(Cortes and Vapnik <a href="#ref-R-vapnik">1995</a>)</span>. Kernel methods convert linear algorithms for use on nonlinear data by projecting the input data into a high dimensional feature space in which a linear solution is found.</p>
<div id="svr-to-predict-travel-time-for-urban-road" class="section level3">
<h3><span class="header-section-number">4.2.1</span> SVR to Predict Travel Time for Urban Road</h3>
<p>The idea of the regression problem is to determine a function that can approximate (or predict) future values accurately. The generic <em>SVR estimating function</em> takes the form:</p>
<span class="math display" id="eq:svm1">\[\begin{equation} 
\operatorname {f(x)} = (w.\Phi(x)) = b
 \tag{4.1}
\end{equation}\]</span>
<p>Where <span class="math inline">\(w\subset R^{n}\)</span>, <span class="math inline">\(b\subset R\)</span> and <span class="math inline">\(\Phi\)</span> denotes a nonlinear transformation from <span class="math inline">\(R^{n}\)</span>, to high-dimensional space. The kernel function applied to the study is the <em>Gaussian radial basis function</em> (RBF):</p>
<span class="math display" id="eq:svm2">\[\begin{equation} 
\displaystyle K(\mathbf {x_i} ,\mathbf {y_i} )=\exp \left(-{\frac {\|\mathbf {x_i} -\mathbf {y_i} \|^{2}}{2\sigma ^{2}}}\right)
 \tag{4.2}
\end{equation}\]</span>
<p>Where <span class="math inline">\(\sigma &gt; 0\)</span> is the kernel bandwidth.</p>
<p>SVR tries to find a function <span class="math inline">\(f(x)\)</span> where the predicted values are at most <span class="math inline">\(\in\)</span> from the observed values <span class="math inline">\(y_i\)</span>, fitting inside a tube of width <span class="math inline">\(2\in\)</span>.</p>
<div class="figure" style="text-align: center"><span id="fig:svrtube"></span>
<img src="Images/svr_tube.png" alt="SVR predictive function*[source: @R-wu]*" width="50%" />
<p class="caption">
Figure 4.3: SVR predictive function<em><span class="citation">(source: Wu and Lee <a href="#ref-R-wu">2004</a>)</span></em>
</p>
</div>
<span class="math display" id="eq:svm3">\[\begin{equation}
\text{ minimize } \frac{1}{2} \|w\|^2
 \tag{4.3}
\end{equation}\]</span>
<span class="math display" id="eq:svm4">\[\begin{equation}
\text{ subject to } 
\begin{cases}
                    y_i - \langle w, x_i \rangle  - b \le \varepsilon,  \\
                    \langle w, x_i \rangle + b - y_i \le \varepsilon,
                   \end{cases}

 \tag{4.4}
\end{equation}\]</span>
<p>where <span class="math inline">\(x_i\)</span> is a training sample with target value <span class="math inline">\(y_i\)</span>. The inner product plus intercept <span class="math inline">\(\langle w, x_i \rangle + b\)</span> is the prediction for that sample, and <span class="math inline">\(\in\)</span> is a free parameter that serves as a threshold. All predictions have to be within a <span class="math inline">\(\in\)</span> range of the true predictions. Slack variables are usually added into the above to allow for errors and approximation in the case the problem is infeasible.</p>
<p>To train the SVR models, a better classification could be achieved by optimizing the parameters in some way. There are two parameters to train: the kernel parameter <code>sigma</code> and the constant <code>c</code>. The former is set to automatic by default. The constant <code>C</code> determines the amount of allowable error in the model, controlling the trade-off between model complexity and error. The value of <code>c</code> is not estimated and is set to 1. A large <code>c</code> assigns higher penalties to errors so that the regression is trained to minimize error with lower generalization, while a small <code>c</code> assigns fewer penalties to errors.</p>
<p>Allowing a large number of errors, the model would be less complex, thus providing higher generalization ability. On the other hand, If <code>c</code> is set with a high number, SVR would not allow the occurrence of any error and results would lead to a complex model. Therefore, it is worthwhile testing some values of <code>c</code> to see how they affect the results. Finally, <code>epsilon</code> determines the width of the tube.</p>
</div>
</div>
<div id="the-modeling-and-solving-approach-1" class="section level2">
<h2><span class="header-section-number">4.3</span> The Modeling and Solving Approach</h2>
<p>The first part of the experiment is to train the dataset. The data have been divided into training (80%) and testing (20%) data. Four road links have been aleatory chosen to train the model: IDs <em>2112</em>, <em>2061</em>, <em>473</em> and <em>2087</em>. The strategy applied to the model training procedure is composed of four steps presented on Figure <a href="support-vector-regression.html#fig:chunk-procedure">4.4</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:chunk-procedure"></span>
<img src="Images/procedure.png" alt="Model training procedure steps." width="100%" />
<p class="caption">
Figure 4.4: Model training procedure steps.
</p>
</div>
<p>To train the data, the package called <code>caret</code> (short for classification and regression training) has been used. The library provides a set of functions that attempt to streamline the process for creating predictive models.<a href="#fn12" class="footnoteRef" id="fnref12"><sup>12</sup></a></p>
<p><strong>Step 1: Build a time series SVR Model</strong></p>
<p>The first step is to rearrange the series in order to build a time series SVR model by means of a temporal autoregressive structure. The moving average uses past forecast errors to forecast future values of the series.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">load</span>(<span class="st">&quot;forecasting.RData&quot;</span>) <span class="co">#first load the data to the space environement</span>

<span class="kw">library</span>(kernlab)  <span class="co"># install if necessary using &#39;install.packages&#39;</span>
<span class="kw">library</span>(caret) <span class="co"># install if necessary using &#39;install.packages&#39;</span></code></pre></div>
<p>To build the model, we will use a temporal autoregressive structure. We use <code>embed()</code> function to rearrange the series. This means we will forecast the future TT at a single location (link=2112) as a function of a subset of previous TT observations.</p>
<p>The resulting matrix can be used for one step ahead forecasting by using the first three columns as the independent variables and the fourth column as the dependent variable. In order to perform parameters tuning with k-fold cross-validation, three different embedding dimensions <em>q</em> have been applied: <em>q=3</em>: <em>q=4</em> and <em>q=5</em>.</p>
<p>We will use the <code>embed()</code> to rearrange the series in this way:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># embed: function to rearrange the series</span>
m &lt;-<span class="st"> </span><span class="dv">3</span> 
data.<span class="dv">2112</span> &lt;-<span class="st"> </span><span class="kw">embed</span>(<span class="dt">x =</span> UJT [,<span class="st">&quot;2112&quot;</span>], <span class="dt">dimension =</span> m<span class="op">+</span><span class="dv">1</span>) 
<span class="kw">colnames</span>(data.<span class="dv">2112</span>) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;y_t-3&quot;</span>,<span class="st">&quot;y_t-2&quot;</span>,<span class="st">&quot;y_t-1&quot;</span>,<span class="st">&quot;y_t&quot;</span>)</code></pre></div>
<p>The function embeds the time series <code>x</code> into a low-dimensional euclidean space. This way, it is possible to forecast the future temperature as a function of a subset of previous temperature observations. Each row of the resulting matrix consists of sequences <span class="math inline">\(x[t], x[t-1], ..., x[t-dimension+1]\)</span>, where <span class="math inline">\(t\)</span> is the original index of <span class="math inline">\(x\)</span>. If <span class="math inline">\(x\)</span> is a matrix, (that is <span class="math inline">\(x\)</span> contains more than one variable) then <span class="math inline">\(x[t]\)</span> consists of the observation on each variable.</p>
<p>The resulting matrix can be inspected with the funcion <code>head ()</code> which returns the first or last parts of the matrix. You will see that each row contains four observations, which are four consecutive TT observations. The first row of the matrix contains the first four observations from <code>UJT[,&quot;2112&quot;]</code> in reverse order ( for example column 4 is the first observation and column 1 is the 4<sup>th</sup> observation). In the second row, the first observation is removed from column 4 and the 5<sup>th</sup> observation is added to column 1. This matrix can be used for one step ahead forecasting by using the first three columns as the independent variables and the fourth column as the dependent variable.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Returns the first or last parts of a vector, matrix, table, data frame or function</span>
<span class="kw">head</span> (data.<span class="dv">2112</span>) </code></pre></div>
<pre><code>##          y_t-3     y_t-2     y_t-1        y_t
## [1,] 0.1212617 0.1152238 0.1101922 0.09610367
## [2,] 0.1207585 0.1212617 0.1152238 0.11019217
## [3,] 0.1061669 0.1207585 0.1212617 0.11522378
## [4,] 0.1111985 0.1061669 0.1207585 0.12126170
## [5,] 0.1368597 0.1111985 0.1061669 0.12075854
## [6,] 0.1157269 0.1368597 0.1111985 0.10616689</code></pre>
<p>The same procedure could be applied to the other dimensions <em>q=4</em> and <em>q=5</em>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m &lt;-<span class="st"> </span><span class="dv">4</span>
data.2112_d2 &lt;-<span class="st"> </span><span class="kw">embed</span>(<span class="dt">x =</span> UJT [,<span class="st">&quot;2112&quot;</span>], <span class="dt">dimension =</span> m<span class="op">+</span><span class="dv">1</span>) 
<span class="kw">colnames</span>(data.2112_d2) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;y_t-4&quot;</span> , <span class="st">&quot;y_t-3&quot;</span>,<span class="st">&quot;y_t-2&quot;</span>,<span class="st">&quot;y_t-1&quot;</span>,<span class="st">&quot;y_t&quot;</span>)

m &lt;-<span class="st"> </span><span class="dv">5</span>
data.2112_d3 &lt;-<span class="st"> </span><span class="kw">embed</span>(<span class="dt">x =</span> UJT [,<span class="st">&quot;2112&quot;</span>], <span class="dt">dimension =</span> m<span class="op">+</span><span class="dv">1</span>) 
<span class="kw">colnames</span>(data.2112_d3) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;y_t-5&quot;</span> , <span class="st">&quot;y_t-4&quot;</span> , <span class="st">&quot;y_t-3&quot;</span>,<span class="st">&quot;y_t-2&quot;</span>,<span class="st">&quot;y_t-1&quot;</span>,<span class="st">&quot;y_t&quot;</span>)</code></pre></div>
<p><strong>Step 2: Divide data into training and testing sets</strong></p>
<p>The next step is to divide the embedded data into training and testing sets. The subset of training data is used to fit the model and the testing set to estimate model fitness. It should be noted that in temporal data there is a continuity component whereas it is important to take into account the ordering of time.</p>
<p>That said, based on a continuous data series, the first 80% data of the months have to be selected to train the model and the remaining 20% data has been selected to test the model. Hence, the particular training set <span class="math inline">\(S= {(X_i,y_i), …,(X_i,y_i)}\)</span> has been drawn from the distribution of the random vectors <span class="math inline">\(y\)</span> and <span class="math inline">\(X\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># divide data into training and testing sets - Dimension 1</span>
split &lt;-<span class="st"> </span><span class="dv">4143</span> <span class="co"># UJT row to separe training and testing data</span>
yTrain &lt;-<span class="st"> </span>data.<span class="dv">2112</span> [<span class="dv">1</span><span class="op">:</span>split,<span class="dv">1</span>]
XTrain &lt;-<span class="st"> </span>data.<span class="dv">2112</span> [<span class="dv">1</span><span class="op">:</span>split,<span class="op">-</span><span class="dv">1</span>]
yTest &lt;-<span class="st"> </span>data.<span class="dv">2112</span> [(split<span class="op">+</span><span class="dv">1</span>)<span class="op">:</span><span class="kw">nrow</span>(data.<span class="dv">2112</span> ),<span class="dv">1</span>]
XTest &lt;-<span class="st"> </span>data.<span class="dv">2112</span> [(split<span class="op">+</span><span class="dv">1</span>)<span class="op">:</span><span class="kw">nrow</span>(data.<span class="dv">2112</span> ),<span class="op">-</span><span class="dv">1</span>]

<span class="co"># divide data into training and testing sets - Dimension 2</span>
split_d2 &lt;-<span class="st"> </span><span class="dv">4143</span> <span class="co">#UJT row to separe training and testing data</span>
yTrain_d2 &lt;-<span class="st"> </span>data.2112_d2 [<span class="dv">1</span><span class="op">:</span>split,<span class="dv">1</span>]
XTrain_d2 &lt;-<span class="st"> </span>data.2112_d2 [<span class="dv">1</span><span class="op">:</span>split,<span class="op">-</span><span class="dv">1</span>]
yTest_d2 &lt;-<span class="st"> </span>data.2112_d2 [(split<span class="op">+</span><span class="dv">1</span>)<span class="op">:</span><span class="kw">nrow</span>(data.2112_d2 ),<span class="dv">1</span>]
XTest_d2 &lt;-<span class="st"> </span>data.2112_d2 [(split<span class="op">+</span><span class="dv">1</span>)<span class="op">:</span><span class="kw">nrow</span>(data.2112_d2 ),<span class="op">-</span><span class="dv">1</span>]

<span class="co"># divide data into training and testing sets - Dimension 3</span>
split_d3 &lt;-<span class="st"> </span><span class="dv">4143</span> <span class="co"># UJT row to separe training and testing data</span>
yTrain_d3 &lt;-<span class="st"> </span>data.2112_d3 [<span class="dv">1</span><span class="op">:</span>split,<span class="dv">1</span>]
XTrain_d3 &lt;-<span class="st"> </span>data.2112_d3 [<span class="dv">1</span><span class="op">:</span>split,<span class="op">-</span><span class="dv">1</span>]
yTest_d3 &lt;-<span class="st"> </span>data.2112_d3 [(split<span class="op">+</span><span class="dv">1</span>)<span class="op">:</span><span class="kw">nrow</span>(data.2112_d3 ),<span class="dv">1</span>]
XTest_d3 &lt;-<span class="st"> </span>data.2112_d3 [(split<span class="op">+</span><span class="dv">1</span>)<span class="op">:</span><span class="kw">nrow</span>(data.2112_d3 ),<span class="op">-</span><span class="dv">1</span>]</code></pre></div>
<p><strong>Step 3. Perform parameters tuning with k-fold cross validation</strong></p>
<p>To determine the values of the tuning parameters, one approach is to use a <em>resampling</em> to estimate how well the model performs on the training set. There are different types of resampling methods being <em>k-fold cross-validation</em> one of the most common types<a href="#fn13" class="footnoteRef" id="fnref13"><sup>13</sup></a>. The process must be repeated many times and the performance estimates from each holdout set are averaged into a final overall estimate of model efficacy such that given the training set, the algorithm produces a prediction function <span class="math inline">\(f(x)= Փ(x_i)\)</span>. For each parameter combination the model fitness is estimated via resampling and the relationship between the tuning parameters and the model performance is evaluated.</p>
<p>The <em>k-fold cross-validation</em> has been applied as resampling method with <span class="math inline">\(k\)</span> set to 5. In k-fold cross validation, the data are randomly partitioned into <em>k-folds</em>. Each fold is left out in turn and the remaining <span class="math inline">\(k-1\)</span> folds are used to train a model and predict its values. The selected model is the one with the best average performance across the k folds<a href="#fn14" class="footnoteRef" id="fnref14"><sup>14</sup></a>. The procedure prevents overfitting to a subset of the training data.</p>
<p>The <code>caret</code> package is used to perform the cross validation with a radial basis function kernel applied for parameter tuning. There are two tuning parameters: the radial basis function scale parameter bandwidth <span class="math inline">\(α\)</span>, and the cost value associated with support vectors. To train the model we will use k-fold cross validation, with <span class="math inline">\(k\)</span> set to 5.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#To train the model we will use k-fold cross validation, with k set to 5</span>
ctrl &lt;-<span class="st"> </span><span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>, <span class="dt">number=</span><span class="dv">5</span>) </code></pre></div>
<p>To test (at present, caret only allows <span class="math inline">\(sigma\)</span> and <span class="math inline">\(C\)</span> to be trained in this way so we cannot optimise over epsilon automatically) and train the model, a grid of parameters must be created.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Create a grid of parameters to test and train the model with dimension 1</span>
SVRGridCoarse &lt;-<span class="st"> </span><span class="kw">expand.grid</span>(<span class="dt">.sigma=</span><span class="kw">c</span>(<span class="fl">0.001</span>, <span class="fl">0.01</span>, <span class="fl">0.1</span>), <span class="dt">.C=</span><span class="kw">c</span>(<span class="dv">10</span>,<span class="dv">100</span>,<span class="dv">1000</span>))
SVRFitCoarse &lt;-<span class="st"> </span><span class="kw">train</span>(XTrain, yTrain, <span class="dt">method=</span><span class="st">&quot;svmRadial&quot;</span>, <span class="dt">tuneGrid=</span>SVRGridCoarse, <span class="dt">trControl=</span>ctrl, <span class="dt">type=</span><span class="st">&quot;eps-svr&quot;</span>)

<span class="co"># Display results</span>
SVRFitCoarse </code></pre></div>
<pre><code>## Support Vector Machines with Radial Basis Function Kernel 
## 
## 4143 samples
##    3 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 3313, 3315, 3315, 3315, 3314 
## Resampling results across tuning parameters:
## 
##   sigma  C     RMSE        Rsquared   MAE       
##   0.001    10  0.01855788  0.7449160  0.01395675
##   0.001   100  0.01838884  0.7497854  0.01383987
##   0.001  1000  0.01831466  0.7516992  0.01375272
##   0.010    10  0.01834316  0.7509672  0.01377398
##   0.010   100  0.01841338  0.7491764  0.01381130
##   0.010  1000  0.01844320  0.7484456  0.01383601
##   0.100    10  0.01854439  0.7456133  0.01388359
##   0.100   100  0.01887267  0.7366928  0.01400829
##   0.100  1000  0.01959266  0.7178368  0.01415667
## 
## RMSE was used to select the optimal model using the smallest value.
## The final values used for the model were sigma = 0.001 and C = 1000.</code></pre>
<p>Results are given for each of the parameter combinations in terms of the root mean squared error (RMSE), R squared and Mean Absolute Error (MAE). We can view the results visually using the <code>ggplot()</code> function, as displayed in figure <a href="support-vector-regression.html#fig:trainingerrors">4.5</a>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(SVRFitCoarse)</code></pre></div>
<div class="figure"><span id="fig:trainingerrors"></span>
<img src="Images/trainingerrors-1.png" alt="Training errors for the SVR model with different parameter combinations" width="672" />
<p class="caption">
Figure 4.5: Training errors for the SVR model with different parameter combinations
</p>
</div>
<p>The plot shows that the value of sigma=0.1 and C=1000 provide the best model performance. However, we can refine the grid to see if we can gain further improvements in performance.</p>
<p>To test the best performance, we might differentiate the values of the embedding dimension with the three different autoregressive orders have been used: <span class="math inline">\(q=3\)</span>; <span class="math inline">\(q=4\)</span> and <span class="math inline">\(q=5\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Create a grid of parameters to test and train the model with dimension 2</span>
SVRGridCoarse_d2 &lt;-<span class="st"> </span><span class="kw">expand.grid</span>(<span class="dt">.sigma=</span><span class="kw">c</span>(<span class="fl">0.001</span>, <span class="fl">0.01</span>, <span class="fl">0.1</span>), <span class="dt">.C=</span><span class="kw">c</span>(<span class="dv">10</span>,<span class="dv">100</span>,<span class="dv">1000</span>))
SVRFitCoarse_d2 &lt;-<span class="st"> </span><span class="kw">train</span>(XTrain_d2, yTrain_d2, <span class="dt">method=</span><span class="st">&quot;svmRadial&quot;</span>, <span class="dt">tuneGrid=</span>SVRGridCoarse, <span class="dt">trControl=</span>ctrl, <span class="dt">type=</span><span class="st">&quot;eps-svr&quot;</span>)
SVRFitCoarse_d2</code></pre></div>
<pre><code>## Support Vector Machines with Radial Basis Function Kernel 
## 
## 4143 samples
##    4 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 3313, 3314, 3314, 3316, 3315 
## Resampling results across tuning parameters:
## 
##   sigma  C     RMSE        Rsquared   MAE       
##   0.001    10  0.01844992  0.7485321  0.01383254
##   0.001   100  0.01822262  0.7548238  0.01367620
##   0.001  1000  0.01816424  0.7561625  0.01361397
##   0.010    10  0.01819337  0.7551913  0.01363658
##   0.010   100  0.01821017  0.7550758  0.01364336
##   0.010  1000  0.01819567  0.7555293  0.01365335
##   0.100    10  0.01838699  0.7497893  0.01371988
##   0.100   100  0.01867380  0.7415464  0.01388985
##   0.100  1000  0.02000167  0.7053679  0.01426356
## 
## RMSE was used to select the optimal model using the smallest value.
## The final values used for the model were sigma = 0.001 and C = 1000.</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Create a grid of parameters to test and train the modelwith dimension 3</span>
SVRGridCoarse_d3 &lt;-<span class="st"> </span><span class="kw">expand.grid</span>(<span class="dt">.sigma=</span><span class="kw">c</span>(<span class="fl">0.001</span>, <span class="fl">0.01</span>, <span class="fl">0.1</span>), <span class="dt">.C=</span><span class="kw">c</span>(<span class="dv">10</span>,<span class="dv">100</span>,<span class="dv">1000</span>))
SVRFitCoarse_d3 &lt;-<span class="st"> </span><span class="kw">train</span>(XTrain_d3, yTrain_d3, <span class="dt">method=</span><span class="st">&quot;svmRadial&quot;</span>, <span class="dt">tuneGrid=</span>SVRGridCoarse, <span class="dt">trControl=</span>ctrl, <span class="dt">type=</span><span class="st">&quot;eps-svr&quot;</span>)
SVRFitCoarse_d3</code></pre></div>
<pre><code>## Support Vector Machines with Radial Basis Function Kernel 
## 
## 4143 samples
##    5 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 3312, 3315, 3315, 3314, 3316 
## Resampling results across tuning parameters:
## 
##   sigma  C     RMSE        Rsquared   MAE       
##   0.001    10  0.01837042  0.7505541  0.01377700
##   0.001   100  0.01816826  0.7563109  0.01362338
##   0.001  1000  0.01817603  0.7559690  0.01360780
##   0.010    10  0.01822833  0.7545035  0.01363603
##   0.010   100  0.01826660  0.7537413  0.01367489
##   0.010  1000  0.01833533  0.7522191  0.01373710
##   0.100    10  0.01856849  0.7463215  0.01388551
##   0.100   100  0.01933124  0.7263834  0.01423794
##   0.100  1000  0.02172044  0.6724945  0.01492969
## 
## RMSE was used to select the optimal model using the smallest value.
## The final values used for the model were sigma = 0.001 and C = 100.</code></pre>
<p>We now can use <code>ggplot()</code> to plot the results.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(SVRFitCoarse_d2)
<span class="kw">ggplot</span>(SVRFitCoarse_d3)</code></pre></div>
<div class="figure"><span id="fig:unnamed-chunk-19"></span>
<img src="Images/unnamed-chunk-19-1.png" alt="Training errors for the SVR model with different parameter combinations" width="672" /><img src="_main_files/figure-html/unnamed-chunk-19-2.png" alt="Training errors for the SVR model with different parameter combinations" width="672" />
<p class="caption">
Figure 4.6: Training errors for the SVR model with different parameter combinations
</p>
</div>
<p>For all dimensions, <span class="math inline">\(sigma = 0.001\)</span> and <span class="math inline">\(C = 1000\)</span> present the best outome, whereas <span class="math inline">\(q = 4\)</span> provides the best fit with <span class="math inline">\(RMSE = 0.01813995\)</span>. However, it should be noted that smaller value of sigma could produce a better result.</p>
<p>We may achieve a better classification if we try to optimise the parameters in some way. There are two parameters to train: the kernel parameter <span class="math inline">\(sigma\)</span> and the constant <span class="math inline">\(C\)</span>. <span class="math inline">\(Sigma\)</span> is set <code>automatic</code> by default. The value of <span class="math inline">\(C\)</span> is not estimated and is arbitrary set to <span class="math inline">\(a\)</span>. Therefore, it is important to test some more values for <span class="math inline">\(C\)</span>.</p>
<p>We can refine the grid to see if we can gain further improvements in performance.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Testing the grid to see if we can gain further improvements in performance: diferent sigma for d1</span>
SVRGridFine &lt;-<span class="st"> </span><span class="kw">expand.grid</span>(<span class="dt">.sigma=</span><span class="kw">c</span>(<span class="fl">0.005</span>, <span class="fl">0.05</span>, <span class="fl">0.5</span>), <span class="dt">.C=</span><span class="kw">c</span>(<span class="dv">5</span>,<span class="dv">50</span>,<span class="dv">500</span>))
<span class="kw">system.time</span>(SVRFitFine &lt;-<span class="st"> </span><span class="kw">train</span>(XTrain, yTrain, <span class="dt">method=</span><span class="st">&quot;svmRadial&quot;</span>, <span class="dt">tuneGrid=</span>SVRGridFine, 
                                <span class="dt">trControl=</span>ctrl, <span class="dt">type=</span><span class="st">&quot;eps-svr&quot;</span>))</code></pre></div>
<pre><code>##    user  system elapsed 
##  155.50    4.16  160.37</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Display results</span>
SVRFitFine</code></pre></div>
<pre><code>## Support Vector Machines with Radial Basis Function Kernel 
## 
## 4143 samples
##    3 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 3314, 3314, 3314, 3314, 3316 
## Resampling results across tuning parameters:
## 
##   sigma  C    RMSE        Rsquared   MAE       
##   0.005    5  0.01842091  0.7492711  0.01386240
##   0.005   50  0.01833551  0.7513841  0.01377070
##   0.005  500  0.01834914  0.7509496  0.01376983
##   0.050    5  0.01836352  0.7506480  0.01379302
##   0.050   50  0.01844122  0.7486064  0.01381828
##   0.050  500  0.01949416  0.7230844  0.01400462
##   0.500    5  0.01881759  0.7380230  0.01399943
##   0.500   50  0.01966634  0.7145673  0.01431567
##   0.500  500  0.02148766  0.6668612  0.01488730
## 
## RMSE was used to select the optimal model using the smallest value.
## The final values used for the model were sigma = 0.005 and C = 50.</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Plot the results</span>
<span class="kw">ggplot</span>(SVRFitFine)</code></pre></div>
<div class="figure"><span id="fig:unnamed-chunk-21"></span>
<img src="Images/unnamed-chunk-21-1.png" alt="Training errors for the SVR model with different parameter combinations" width="672" />
<p class="caption">
Figure 4.7: Training errors for the SVR model with different parameter combinations
</p>
</div>
<p><strong>Step 4: Select the model and apply predictive analysis</strong></p>
<p>The effectiveness of a model is commonly measured using a single statistic. For models predicting a numeric outcome, the fitness statistic might be the <em>root mean squared error</em> (RMSE) or the <em>coefficient of determination</em> (r<sup>2</sup>). For classification, where a discrete outcome is being predicted, the error rate might be an appropriate measure.</p>
<p>Examining the results considering the RMSE index, the best performance can be pointed out for the road link.</p>
<p>The model object for the best model can be accessed by:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">SVRFitFine<span class="op">$</span>finalModel</code></pre></div>
<pre><code>## Support Vector Machine object of class &quot;ksvm&quot; 
## 
## SV type: eps-svr  (regression) 
##  parameter : epsilon = 0.1  cost C = 50 
## 
## Gaussian Radial Basis kernel function. 
##  Hyperparameter : sigma =  0.005 
## 
## Number of Support Vectors : 3359 
## 
## Objective Function Value : -58828.6 
## Training error : 0.247802</code></pre>
<p>The model for one-step-ahead prediction can be used to plot the results (figure <a href="support-vector-regression.html#fig:prediction">4.8</a>). The model forecasts the TTs reasonably well. For road link 2112, the proportion of points used as support vector has been of 62% from the dataset.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># We can use the model for one-step-ahead prediction and plot the results</span>
yPred &lt;-<span class="st"> </span><span class="kw">predict</span>(SVRFitCoarse_d2, XTest_d2)
<span class="kw">plot</span>(yTest_d2, <span class="dt">type=</span><span class="st">&quot;l&quot;</span>, <span class="dt">xaxt=</span><span class="st">&quot;n&quot;</span>, <span class="dt">xlab=</span><span class="st">&quot;Observations&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;TTs&quot;</span>)
<span class="kw">points</span>(yPred, <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>, <span class="dt">pch=</span><span class="dv">21</span>, <span class="dt">bg=</span><span class="st">&quot;blue&quot;</span>)
<span class="co">#axis(1, at=seq(90, (180*5)+90, 180), labels=unique(dates[(split+1):nrow(data),1]))</span></code></pre></div>
<div class="figure"><span id="fig:prediction"></span>
<img src="Images/prediction-1.png" alt="Observed vs predicted TT using an SVR model" width="672" />
<p class="caption">
Figure 4.8: Observed vs predicted TT using an SVR model
</p>
</div>
<p>Finally, it is also important to check whether any temporal autocorrelation remains in the residuals of the model. The <em>autocorelation function</em> (ACF) is used in time series analyses.</p>
<p>It is good practice to also check whether any temporal autocorrelation remains in the residuals of the model (figure <a href="support-vector-regression.html#fig:grid9">4.9</a>)</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Check whether any temporal autocorrelation remains in the residuals of the model</span>
SVRResidual &lt;-<span class="st"> </span>yTest_d2<span class="op">-</span>yPred
<span class="kw">plot</span>(SVRResidual)</code></pre></div>
<div class="figure"><span id="fig:grid9"></span>
<img src="Images/grid9-1.png" alt="Residual autocorrelation for SVR model" width="672" />
<p class="caption">
Figure 4.9: Residual autocorrelation for SVR model
</p>
</div>
<p>figure <a href="support-vector-regression.html#fig:grid10">4.10</a>) displays differenced autocorrelation function (ACF) plot for the data of the series. The ACF is usually calculated for a set number of lags. The smaller the lag (separation between observations) the greater the correlation between temperatures. The dashed lines are approximate 95% confident interval for the autocorrelation. It can be notted that no significant overall autocorrelation is presented. Additionally, no seasonal pattern could be noted as well.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># To produce an autocorrelation plot</span>
<span class="kw">acf</span>(SVRResidual)</code></pre></div>
<div class="figure"><span id="fig:grid10"></span>
<img src="Images/grid10-1.png" alt="Residual autocorrelation for SVR model" width="672" />
<p class="caption">
Figure 4.10: Residual autocorrelation for SVR model
</p>
</div>
</div>
<div id="chapter-summary-3" class="section level2">
<h2><span class="header-section-number">4.4</span> Chapter Summary</h2>
<p>The basic idea behind the development of SVMs is the same: for a given learning task, with a given finite number of training data, the best generalization performance will be achieved if the right balance is struck between the accuracy attained on that particular training set, and the ability of the algorithm to predict unseen data samples with a minimum of errors.</p>
<p>Finally, it is worth mentioning that the experiment applies time series only. It is possible to create a space-time model using the spatial weight matrix but since the spatio-temporal autocorrelation is weaker than the temporal autocorrelation (as previously discussed on the study), the model has not been applied.</p>
<hr />

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">knitr<span class="op">::</span>opts_chunk<span class="op">$</span><span class="kw">set</span>(<span class="dt">error =</span> <span class="ot">TRUE</span>)</code></pre></div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-R-burges">
<p>Burges, C. J. 1998. “A Tutorial on Support Vector Machines for Pattern Recognition.” <em>Data Mining and Knowledge Discovery</em> 2(2): 121–67.</p>
</div>
<div id="ref-R-cheng2">
<p>Cheng, T, and J. Haworth. 2019. “Spatio-Temporal Data Analysis and Big Data Mining.” <em>University College London</em>.</p>
</div>
<div id="ref-R-vapnik">
<p>Cortes, C., and V. Vapnik. 1995. “Support-Vector Networks.” <em>Machine Learning</em> 20(3): 273–97.</p>
</div>
<div id="ref-R-wu">
<p>Wu, Ho, C. H., and D. T. Lee. 2004. “Travel-Time Prediction with Support Vector Regression.” <em>IEEE Transactions on Intelligent Transportation Systems</em> 5(4): 276–81.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="11">
<li id="fn11"><p>SVR has greater generalization ability and guarantee global minima for a given training data.<a href="support-vector-regression.html#fnref11">↩</a></p></li>
<li id="fn12"><p>For more information, see <a href="http://topepo.github.io/caret/index.html" class="uri">http://topepo.github.io/caret/index.html</a><a href="support-vector-regression.html#fnref12">↩</a></p></li>
<li id="fn13"><p>Other common resampling methods are leave-one-out cross-validation, Monte Carlo cross-validation and the bootstrap<a href="support-vector-regression.html#fnref13">↩</a></p></li>
<li id="fn14"><p>The optimal model is selected based on accuracy statistics.<a href="support-vector-regression.html#fnref14">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="seasonal-and-trend-decomposition-with-loess-forecasting-model-stlf.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="artificial-neural-network.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
